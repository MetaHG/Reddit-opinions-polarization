{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "comments = spark.read.load('../data/sample.parquet')\n",
    "comments = comments.withColumn('created', func.from_unixtime(comments['created_utc'], 'yyyy-MM-dd HH:mm:ss.SS').cast(DateType()))\n",
    "comments.registerTempTable(\"comments\")\n",
    "sc = spark.sparkContext\n",
    "\n",
    "import spacy\n",
    "import gensim\n",
    "from spacy.lang.en import English\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import datetime\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first test, this first LDA algorithm was written. As data input, it takes simply a dataframe of the following data pair as row : the reddit's comment text + the id of the post on which the comment was written. The idea was to only give a selected (column wise) and possibly filtered version of the comments from reddit, and the function would make all the necessary in order to model topic on posts. Although nice for the user side, it proved later to be unoptimal for cluster broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|  post_id|               topic|\n",
      "+---------+--------------------+\n",
      "|t3_5axjwf|already:/ pennsyl...|\n",
      "|t3_5aw066|        excuse least|\n",
      "|t3_5as8g0|    amendment civics|\n",
      "|t3_5aiyva|       school people|\n",
      "|t3_5ap0a7|     military payday|\n",
      "|t3_5apdai|              thanks|\n",
      "|t3_5b5m2v|      people picture|\n",
      "|t3_5b3jm9|       dealer option|\n",
      "|t3_5acf8u|       police arrest|\n",
      "|t3_5a61rd|       bones chicken|\n",
      "|t3_5ah1kd|    people insurance|\n",
      "|t3_5aih4r|        drug looking|\n",
      "|t3_5b6w4x|      complain going|\n",
      "|t3_5akrra|     another appoint|\n",
      "|t3_5b9ik4|       money private|\n",
      "|t3_5abvof|     believing great|\n",
      "|t3_5bbo5w|      murderer extra|\n",
      "|t3_5aqikp|         replacement|\n",
      "|t3_5b7a5q|       humor officer|\n",
      "|t3_5b201j|        larger force|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.date(year=2016, month=10, day=30)\n",
    "end_date = datetime.date(year=2016, month=11, day=7)\n",
    "\n",
    "oct_2016_news_comments = comments.select('link_id','body','created', 'subreddit').filter(comments.created > start_date).filter(comments.created < end_date).filter(comments.subreddit == 'news')\n",
    "\n",
    "def lda_on_comments(dataset, words_per_topic, n_topics, parser=English(), stop_words=en_stop):\n",
    "    '''   \n",
    "    This function performs a LDA (Latent Dirichlet Allocation) model on a set of reddit comments.\n",
    "    Useful for topic modelling/extraction from a reddit post.\n",
    "    Parameters\n",
    "    −−−−−−−−−−\n",
    "    dataset: pyspark RDD or Dataframe, schema should have only three data type : \n",
    "              the post id (link_id), the body of the comment and the creation date in this order.\n",
    "              \n",
    "    words_per_topic: number of words that should constitute a topic per post.\n",
    "    \n",
    "    n_topics: number of topics to extract by post\n",
    "    \n",
    "    parser: the natural language parser used, corresponds to a language normally,\n",
    "            by default english (as it is the most used language on reddit).\n",
    "            should be a parser from the spacy.lang library.\n",
    "    \n",
    "    stop_words: set of words that constitutes stop words (i.e. that should be\n",
    "                removed from the tokens)\n",
    "\n",
    "    Returns\n",
    "    −−−−−−−\n",
    "    A RDD with the following pair of data as rows : (<post_id>, <topic (as a list of words)>)) \n",
    "    '''\n",
    "    #useful functions for preprocessing the data for LDA\n",
    "    def tokenize(text):\n",
    "        lda_tokens = []\n",
    "        tokens = parser(text)\n",
    "        for token in tokens:\n",
    "            if token.orth_.isspace():\n",
    "                continue\n",
    "            elif token.like_url:\n",
    "                lda_tokens.append('URL')\n",
    "            elif token.orth_.startswith('@'):\n",
    "                lda_tokens.append('SCREEN_NAME')\n",
    "            else:\n",
    "                lda_tokens.append(token.lower_)\n",
    "        return lda_tokens\n",
    "\n",
    "    def get_lemma(word):\n",
    "        lemma = wn.morphy(word)\n",
    "        if lemma is None:\n",
    "            return word\n",
    "        else:\n",
    "            return lemma\n",
    "\n",
    "    def get_lemma2(word):\n",
    "        return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "    def prepare_text_for_lda(text):\n",
    "        tokens = tokenize(text)\n",
    "        tokens = [token for token in tokens if len(token) > 4]\n",
    "        tokens = [token for token in tokens if token not in en_stop]\n",
    "        tokens = [get_lemma(token) for token in tokens]\n",
    "        return tokens\n",
    "    \n",
    "    def get_n_topics(text_data):\n",
    "        dictionary = gensim.corpora.Dictionary(text_data)\n",
    "        corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = n_topics, id2word=dictionary, passes=15)\n",
    "        topics = ldamodel.print_topics(num_words=words_per_topic)\n",
    "        return topics\n",
    "    \n",
    "    def extract_key_words(lda_result):\n",
    "        return re.findall(r'\\\"(.*?)\\\"', lda_result)\n",
    "\n",
    "    #detecting type of the input given.\n",
    "    if isinstance(dataset, pyspark.sql.dataframe.DataFrame):\n",
    "        dataset = dataset.rdd\n",
    "    elif not isinstance(dataset, pyspark.rdd.RDD):\n",
    "        raise ValueError('Wrong type of dataset, must be either a pyspark RDD or pyspark DataFrame')\n",
    "    \n",
    "    #TODO : keep the minimum timestamp (r[2] of the dataset) during computations.\n",
    "    \n",
    "    #filtering comments that were removed, to avoid them to pollute the topics extracted\n",
    "    filter_absent_comm = dataset.filter(lambda r: r[1] != '[removed]' and r[1] != '[deleted]')\n",
    "    \n",
    "    #applying text preprocesisng for LDA + filtering all empty sets (without tokens as the result of the LDA preprocessing)\n",
    "    LDA_preprocessed = filter_absent_comm.map(lambda r: (r[0], list(prepare_text_for_lda(r[1])))).filter(lambda r: r[1])\n",
    "    \n",
    "    #groupy every comments by post/thread id.\n",
    "    post_and_list_token = LDA_preprocessed.groupByKey().map(lambda x : (x[0], list(x[1])))\n",
    "    \n",
    "    #generating n topics per post/thread.\n",
    "    res_lda = post_and_list_token.map(lambda r: (r[0],get_n_topics(r[1]))).flatMap(lambda r: [(r[0], t) for t in r[1]])\n",
    "    \n",
    "    return res_lda.map(lambda r: (r[0], ' '.join(extract_key_words(r[1][1])))).toDF().selectExpr(\"_1 as post_id\", \"_2 as topic\")\n",
    "\n",
    "res = lda_on_comments(oct_2016_news_comments.rdd.map(lambda r: r), 2, 1)\n",
    "#hillary_and_trump = res.filter(lambda r: ('trump' in r[1]) or ('hillary' in r[1]) or ('donald' in r[1]) or ('clinton' in r[1]))\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating all comments from one post previous to performing LDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the basic LDA algorithm written previously using gensim and spacy cannot be broadcasted easily to the cluster due to heavy libraries dependencies, I produced this rewritten version, which assumes the dataset given as input is already a combined by post pastiche of comments from reddit (done by condense_post_into_text). Thus, it leaves to lda_on_posts the sole duty of performing the LDA.\n",
    "\n",
    "The condense post_into_text part is purely pyspark manipulations, thus it could be broadcasted to the cluster. While waiting for a full cluster version of LDA, condense_post_into_text could be used with some subreddit and time-window filtering in order to produce a parquet of post text with timestamp that could be processed locally using the lda_on_posts function.\n",
    "\n",
    "Splitting the pyspark and lda part of the previous algorithm also allowed me to have a better way to compute the timestamp of the post, which is useful to model a trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condense_post_into_text(dataset):\n",
    "    '''\n",
    "    Funciton whose purpose is to condensate all comments\n",
    "    of one post (identified by lind_id) into on array\n",
    "    '''\n",
    "    \n",
    "    #keeping only what matters\n",
    "    dataset = dataset.select('link_id', 'body', 'created')\n",
    "\n",
    "    zeroValue = (list(), datetime.date(year=3016, month=12, day=30))\n",
    "    \n",
    "    combFun = lambda l, r: (l[0] + r[0], min(l[1], r[1]))\n",
    "\n",
    "    seqFun = lambda prev, curr: (prev[0] + [curr[0]], prev[1] if prev[1] < curr[1] else curr[1])\n",
    "    \n",
    "    #removing post that have been deleted/removed (thus hold no more information)\n",
    "    filtered = dataset.filter(dataset.body != '[removed]').filter(dataset.body != '[deleted]').filter(dataset.body != '')\n",
    "\n",
    "    post_and_list_token = filtered.rdd.map(lambda r: (r[0], (r[1], r[2]))).aggregateByKey(zeroValue, seqFun, combFun)\n",
    "    \n",
    "    return post_and_list_token.map(lambda r: (r[0], r[1][0], r[1][1])).toDF().selectExpr(\"_1 as post_id\", \"_2 as text\",\"_3 as created\")\n",
    "\n",
    "\n",
    "def lda_on_posts(dataset, n_topics, n_words):\n",
    "    #The post entry in the df/rdd is an id, followed by a disordered list of \n",
    "    def tokenize(text):\n",
    "        lda_tokens = []\n",
    "        tokens = parser(text)\n",
    "        for token in tokens:\n",
    "            if token.orth_.isspace():\n",
    "                continue\n",
    "            elif token.like_url:\n",
    "                lda_tokens.append('URL')\n",
    "            elif token.orth_.startswith('@'):\n",
    "                lda_tokens.append('SCREEN_NAME')\n",
    "            else:\n",
    "                lda_tokens.append(token.lower_)\n",
    "        return lda_tokens\n",
    "\n",
    "    def get_lemma(word):\n",
    "        lemma = wn.morphy(word)\n",
    "        if lemma is None:\n",
    "            return word\n",
    "        else:\n",
    "            return lemma\n",
    "\n",
    "    def get_lemma2(word):\n",
    "        return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "    def prepare_text_for_lda(text):\n",
    "        tokens = tokenize(text)\n",
    "        tokens = [token for token in tokens if len(token) > 4]\n",
    "        tokens = [token for token in tokens if token not in en_stop]\n",
    "        tokens = [get_lemma(token) for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    def get_n_topics(text_data):\n",
    "        dictionary = gensim.corpora.Dictionary(text_data)\n",
    "        \n",
    "        corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = n_topics, id2word=dictionary, passes=15)\n",
    "        topics = ldamodel.print_topics(num_words=n_words)\n",
    "        return topics\n",
    "\n",
    "    def extract_key_words(lda_result):\n",
    "        return re.findall(r'\\\"(.*?)\\\"', lda_result)\n",
    "    \n",
    "    #applying text preprocesisng for LDA + filtering all empty sets (without tokens as the result of the LDA preprocessing)\n",
    "    preprocessed = dataset.rdd.map(lambda r: (r[0], [prepare_text_for_lda(comm) for comm in r[1]], r[2]))\n",
    "    \n",
    "    #this may seem a bit unnecesseray, but it prevents rare posts whose text has been cleansed by stop-words.\n",
    "    preprocessed_filtered = preprocessed.filter(lambda r: r[1]).filter(lambda r: r[1][0])\n",
    "    \n",
    "    #generating n topics per post/thread.\n",
    "    res = preprocessed_filtered.map(lambda r: (r[0],get_n_topics(r[1]), r[2])).flatMap(lambda r: [(r[0], t, r[2]) for t in r[1]])\n",
    "    \n",
    "    return res.map(lambda r: (r[0], ' '.join(extract_key_words(r[1][1])), r[2])).toDF().selectExpr(\"_1 as post_id\", \"_2 as topic\", \"_3 as created\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend Analysis using 1 topic by post, and 1 word per topic\n",
    "\n",
    "Let's try the LDA on all comments from the subreddit \"news\" during the month preceding the 2016 US presidential election. The most simplest and general topic modelling that can be done, is asking the LDA to produce only one topic consisting of 1 word per post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "oct_comments = spark.read.parquet(\"../data/oct_2016_news_comment.parquet\")\n",
    "LDA_result = lda_on_posts(condense_post_into_text(oct_comments), 1, 1)\n",
    "lda_result_df = LDA_result.toPandas().set_index('post_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people          395\n",
       "would           135\n",
       "police           77\n",
       "article          50\n",
       "school           41\n",
       "trump            38\n",
       "child            38\n",
       "woman            35\n",
       "clown            33\n",
       "pipeline         33\n",
       "could            29\n",
       "state            28\n",
       "attack           26\n",
       "actually         25\n",
       "black            25\n",
       "right            24\n",
       "phone            24\n",
       "american         23\n",
       "crime            22\n",
       "russia           22\n",
       "officer          22\n",
       "money            21\n",
       "going            21\n",
       "animal           20\n",
       "country          20\n",
       "anything         20\n",
       "charge           20\n",
       "think            19\n",
       "prison           19\n",
       "happen           18\n",
       "               ... \n",
       "services          1\n",
       "reoffend          1\n",
       "certainly         1\n",
       "fentanyl          1\n",
       "buyout            1\n",
       "timetable         1\n",
       "continue          1\n",
       "bar               1\n",
       "monitoring        1\n",
       "arrogance         1\n",
       "counseling        1\n",
       "resurrection      1\n",
       "minority          1\n",
       "protestors        1\n",
       "harpy             1\n",
       "general           1\n",
       "laugh             1\n",
       "hoover            1\n",
       "attach            1\n",
       "mentally          1\n",
       "without           1\n",
       "operational       1\n",
       "greengold         1\n",
       "activist          1\n",
       "wales             1\n",
       "environment       1\n",
       "insult            1\n",
       "poland            1\n",
       "incoming          1\n",
       "impact            1\n",
       "Name: topic, Length: 2004, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_df = lda_result_df['topic'].value_counts()\n",
    "frequency_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only one topic limited to one word, we obviously generalize a bit too much the topic of the discussion and get very broad subject such as \"people\" as the most frequent topic. Of course, most of the discussion of news might be based on people, so it is not really a surprise.\n",
    "\n",
    "### TODO : time series graph showing frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend Analysis using 2 topic by post, and 3 word per topic\n",
    "\n",
    "Let's try to get some more fine grained topics, this time producing 2 topic with 3 words each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_result = lda_on_posts(condense_post_into_text(oct_comments), 2, 3)\n",
    "lda_result_df = LDA_result.toPandas().set_index('post_id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
