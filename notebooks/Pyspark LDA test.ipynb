{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources : \n",
    "[1](https://medium.com/@connectwithghosh/topic-modelling-with-latent-dirichlet-allocation-lda-in-pyspark-2cb3ebd5678e)\n",
    "[2](https://stackoverflow.com/questions/42051184/latent-dirichlet-allocation-lda-in-spark#)\n",
    "[3](https://gist.github.com/Bergvca/a59b127afe46c1c1c479)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "en_stop = set(stopwords.words('english'))\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import datetime\n",
    "import re as re\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.mllib.clustering import LDA, LDAModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------+---------+\n",
      "|  link_id|                body|   created|subreddit|\n",
      "+---------+--------------------+----------+---------+\n",
      "|t3_57m54q|&gt; You also don...|2016-10-15|     news|\n",
      "|t3_57gfbu|I don't know if t...|2016-10-15|     news|\n",
      "|t3_57m54q| How can she slap!?!|2016-10-15|     news|\n",
      "|t3_57ljf8|It's almost impos...|2016-10-15|     news|\n",
      "|t3_57i95x|           [deleted]|2016-10-15|     news|\n",
      "|t3_57m54q|i want to sue god...|2016-10-15|     news|\n",
      "|t3_57i5zw|Are you an accoun...|2016-10-15|     news|\n",
      "|t3_57m54q|Good luck to you ...|2016-10-15|     news|\n",
      "|t3_57m54q|Except Jack Danie...|2016-10-15|     news|\n",
      "|t3_57m54q|Well let me know ...|2016-10-15|     news|\n",
      "|t3_57m54q|Ok.. I think he a...|2016-10-15|     news|\n",
      "|t3_57fn69|           [deleted]|2016-10-15|     news|\n",
      "|t3_57m54q|Because he's not ...|2016-10-15|     news|\n",
      "|t3_57it6q|It precludes lice...|2016-10-15|     news|\n",
      "|t3_57i5zw|The problem is so...|2016-10-15|     news|\n",
      "|t3_57m54q|Not exactly. One ...|2016-10-15|     news|\n",
      "|t3_57m54q|It doesn't stop p...|2016-10-15|     news|\n",
      "|t3_57m54q|You probably woul...|2016-10-15|     news|\n",
      "|t3_57lzov|Well he probably ...|2016-10-15|     news|\n",
      "|t3_57m54q|I do not know man...|2016-10-15|     news|\n",
      "+---------+--------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.load('../data/oct_2016_news_comment.parquet')\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condense_post_into_text(dataset):\n",
    "    '''\n",
    "    Function whose purpose is to condensate all comments\n",
    "    of one post (identified by link_id) into one array per post \n",
    "    in the resulting dataframe. Transform the data into a format\n",
    "    that is useful to perform LDA on.\n",
    "    '''\n",
    "    #keeping only what matters\n",
    "    dataset = dataset.select('link_id', 'body', 'created')\n",
    "    \n",
    "    #defining the three element used in aggregate by key to concatenate\n",
    "    #comments, and keep the earliest date.\n",
    "    zeroValue = (list(), datetime.date(year=3016, month=12, day=30))\n",
    "    combFun = lambda l, r: (l[0] + r[0], min(l[1], r[1]))\n",
    "    seqFun = lambda prev, curr: (prev[0] + [curr[0]], prev[1] if prev[1] < curr[1] else curr[1])\n",
    "    \n",
    "    #removing post that have been deleted/removed (thus hold no more information)\n",
    "    filtered = dataset.filter(dataset.body != '[removed]').filter(dataset.body != '[deleted]').filter(dataset.body != '')\n",
    "    post_and_list_token = filtered.rdd.map(lambda r: (r[0], (r[1], r[2]))).aggregateByKey(zeroValue, seqFun, combFun)    \n",
    "    return post_and_list_token.map(lambda r: (r[0], r[1][0], r[1][1]))\n",
    "\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    return word if lemma is None else lemma\n",
    "\n",
    "\n",
    "\n",
    "def reddit_comment_preprocessing(txt):\n",
    "    '''\n",
    "    Take care of doing all the text preprocessing for LDA at the comment level\n",
    "    Only works on english ASCII content. (works on content with accent or such, but filter them out)\n",
    "    '''\n",
    "    #TODO : maybe remove http links.\n",
    "    \n",
    "    #keeping only elements relevant to written speech.\n",
    "    keep_only_letters = lambda s: re.sub('[^a-zA-Z \\']+', '', s)\n",
    "    \n",
    "    #remove mark of genitif from speech (i.e. \"'s\" associated to nouns)\n",
    "    remove_genitive = lambda s: re.sub('(\\'s)+', '', s)\n",
    "    \n",
    "    #tokenizing the texts (removing line break, space and capitalization)\n",
    "    token_comm = re.split(\" \", remove_genitive(keep_only_letters(txt)).strip().lower())\n",
    "    \n",
    "    #removing all words of two letters or less\n",
    "    bigger_w = [x for x in token_comm if len(x) > 2] \n",
    "    \n",
    "    #removing stop_words\n",
    "    wout_sw_w = [x for x in bigger_w if x not in en_stop]\n",
    "    \n",
    "    #get lemma of each word, then return result\n",
    "    return [get_lemma(token) for token in wout_sw_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : It precludes licensing of gun _ownership_\n",
      "After : preclude license gun ownership\n",
      "\n",
      "Before : \n",
      "\n",
      "Why does everyone try to connect guns to cars?\n",
      "After : everyone try connect gun car\n",
      "\n",
      "Before : They believe in \"God's law\" and see man made laws as false.\n",
      "After : believe god law see man make laws false\n",
      "\n",
      "Before : &gt; I was just using the quotes to signify\n",
      "After : using quote signify\n",
      "\n",
      "Before : \n",
      "\n",
      "Obama didn't ban foreign \"assault weapons\"\n",
      "After : obama ban foreign assault weapon\n",
      "\n",
      "Before :   \n",
      "  \n",
      "They also banned importation of surplus 7.62x39.\n",
      "After : also ban importation surplus\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#utilitary function only to test if the preprocessing happens correctly. \n",
    "def prepro_test(txt):\n",
    "    preprocessed = reddit_comment_preprocessing(txt)\n",
    "    print('Before : '+txt)\n",
    "    print('After : '+ ' '.join(preprocessed))\n",
    "    print()\n",
    "    \n",
    "#test strings are actual reddit comment extracts from r/news\n",
    "prepro_test(\"It precludes licensing of gun _ownership_\")\n",
    "prepro_test(\"\\n\\nWhy does everyone try to connect guns to cars?\")\n",
    "prepro_test('They believe in \"God\\'s law\" and see man made laws as false.')\n",
    "prepro_test('&gt; I was just using the quotes to signify')\n",
    "prepro_test('\\n\\nObama didn\\'t ban foreign \"assault weapons\"')\n",
    "prepro_test('  \\n  \\nThey also banned importation of surplus 7.62x39.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = condense_post_into_text(data)\n",
    "\n",
    "#Apply preprocessing for LDA, and add a unique id, usefule later for the LDA computation.\n",
    "preprocessed = dataset.zipWithUniqueId().map(lambda r: (r[0][0], [reddit_comment_preprocessing(comm) for comm in r[0][1]], r[0][2], r[1]))\n",
    "    \n",
    "#this may seem a bit unnecesseray, but it prevents rare posts whose text has been cleansed by stop-words.\n",
    "preprocessed_filtered = preprocessed.filter(lambda r: r[1]).filter(lambda r: r[1][0])\n",
    "\n",
    "#we get a list of list for each post (the second level list is the token list of comments)\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "p_f_flatten = preprocessed_filtered.map(lambda r: (r[0], flatten(r[1]), r[2], r[3])).toDF().selectExpr(\"_1 as post_id\", \"_2 as tokens\", \"_3 as date\", \"_4 as uid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if the uids generated are indeed unique.\n",
    "serie = p_f_flatten.select('uid').toPandas()['uid']\n",
    "serie.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF\n",
    "# TODO : fine tune other parameters\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\", minDF=10.0, vocabSize=5000)\n",
    "cvmodel = cv.fit(p_f_flatten)\n",
    "result_cv = cvmodel.transform(p_f_flatten)\n",
    "\n",
    "num_topics = 2\n",
    "max_iterations = 10\n",
    "wordNumbers = 3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------+---+--------------------+\n",
      "|  post_id|              tokens|      date|uid|        raw_features|\n",
      "+---------+--------------------+----------+---+--------------------+\n",
      "|t3_57it6q|[preclude, licens...|2016-10-14|  0|(5000,[0,1,2,3,4,...|\n",
      "|t3_57m81k|[like, cop, shoot...|2016-10-15|  7|(5000,[0,1,2,3,4,...|\n",
      "|t3_57mcg2|[course, civil, f...|2016-10-15| 14|(5000,[0,1,2,3,4,...|\n",
      "|t3_57kldj|[try, come, back,...|2016-10-15| 21|(5000,[2,4,11,12,...|\n",
      "|t3_57l4u1|[bah, one, saw, s...|2016-10-15| 28|(5000,[0,3,4,6,7,...|\n",
      "|t3_57hfga|[schedule, appear...|2016-10-14| 35|(5000,[1,6,27,73,...|\n",
      "|t3_57kmow|[ukavakavaroo, ye...|2016-10-15| 42|(5000,[1,2,3,4,5,...|\n",
      "|t3_57ma9a|[hope, body, cam,...|2016-10-15| 49|(5000,[1,2,4,8,9,...|\n",
      "|t3_57m4y8|[fake, weed, real...|2016-10-15| 56|(5000,[0,1,2,3,4,...|\n",
      "|t3_57mjlo|[strict, gun, law...|2016-10-15| 63|(5000,[0,1,2,3,4,...|\n",
      "|t3_57msmm|[think, going, bi...|2016-10-15| 70|(5000,[0,1,2,3,4,...|\n",
      "|t3_57lisx|[cant, recognize,...|2016-10-15| 77|(5000,[48,57,59,7...|\n",
      "|t3_574igc|[i'd, literally, ...|2016-10-12| 84|(5000,[0,1,2,3,4,...|\n",
      "|t3_57ba27|[articlegtthe, mo...|2016-10-13| 91|(5000,[0,2,6,7,9,...|\n",
      "|t3_57koov|[racist, blm, has...|2016-10-15| 98|(5000,[0,1,2,3,4,...|\n",
      "|t3_573p96|[want, talk, blan...|2016-10-12|105|(5000,[0,1,2,3,4,...|\n",
      "|t3_57mhhn|[hear, prison, in...|2016-10-15|112|(5000,[1,2,3,5,6,...|\n",
      "|t3_57aaps|[roof, something,...|2016-10-13|119|(5000,[0,1,2,3,4,...|\n",
      "|t3_57j625|[sure, let, label...|2016-10-15|126|(5000,[7,8,13,44,...|\n",
      "|t3_57myf3|[trump, supporter...|2016-10-15|133|(5000,[1,90,148,3...|\n",
      "+---------+--------------------+----------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = result_cv.select('uid', 'raw_features').rdd.map(lambda r: [r[0],Vectors.fromML(r[1])]).cache()\n",
    "result_cv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF\n",
    "#TODO make it work.\n",
    "'''\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv) \n",
    "\n",
    "result_tfidf.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LDA.train(corpus, k=num_topics, maxIterations=max_iterations, optimizer='online')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
