{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources : \n",
    "[General data science article presenting the method](https://medium.com/@connectwithghosh/topic-modelling-with-latent-dirichlet-allocation-lda-in-pyspark-2cb3ebd5678e)\n",
    "\n",
    "[Issue of an user to do LDA on SO](https://stackoverflow.com/questions/42051184/latent-dirichlet-allocation-lda-in-spark#)\n",
    "\n",
    "[Gist of a LDA implementation](https://gist.github.com/Bergvca/a59b127afe46c1c1c479)\n",
    "\n",
    "[SO question about retrieving topic distribution](https://stackoverflow.com/questions/33072449/extract-document-topic-matrix-from-pyspark-lda-model/41515070)\n",
    "\n",
    "[Slides from presentation about recommended parameters for LDA](http://www.phusewiki.org/wiki/images/c/c9/Weizhong_Presentation_CDER_Nov_9th.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "#other words that need to be removed in order to avoid pollution on the topic. (experimental findings on news subreddit)\n",
    "aux_stop_words = ['people', 'would', 'like']\n",
    "\n",
    "en_stop = set(stopwords.words('english')+aux_stop_words)\n",
    "sc = spark.sparkContext\n",
    "sc.broadcast(en_stop)\n",
    "\n",
    "import datetime\n",
    "import re as re\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52354\n",
      "+---------+--------------------+----------+---------+\n",
      "|  link_id|                body|   created|subreddit|\n",
      "+---------+--------------------+----------+---------+\n",
      "|t3_57m54q|Good luck to you ...|2016-10-15|     news|\n",
      "|t3_57m54q|Because he's not ...|2016-10-15|     news|\n",
      "|t3_57m54q|I do not know man...|2016-10-15|     news|\n",
      "|t3_57gfbu|Did the man have ...|2016-10-15|     news|\n",
      "|t3_57m54q|Good ol college l...|2016-10-15|     news|\n",
      "|t3_57m81k|&gt; Noone deserv...|2016-10-15|     news|\n",
      "|t3_57m54q|Well, that's a pr...|2016-10-15|     news|\n",
      "|t3_57m54q|A car cant be con...|2016-10-15|     news|\n",
      "|t3_57m54q|No. I own a singl...|2016-10-15|     news|\n",
      "|t3_57m54q|It's a problem th...|2016-10-15|     news|\n",
      "|t3_57m54q|           [removed]|2016-10-15|     news|\n",
      "|t3_57m54q|Guns have a huge ...|2016-10-15|     news|\n",
      "|t3_57gi47|seriously, a word...|2016-10-15|     news|\n",
      "|t3_57m54q|&gt; I dont see w...|2016-10-15|     news|\n",
      "|t3_57m54q|Unless the name o...|2016-10-15|     news|\n",
      "|t3_57ljf8|You realize most ...|2016-10-15|     news|\n",
      "|t3_57m54q|So if I stab you,...|2016-10-15|     news|\n",
      "|t3_57m54q|He would more lik...|2016-10-15|     news|\n",
      "|t3_57m54q|Why do people kee...|2016-10-15|     news|\n",
      "|t3_57hn6w|Well it's more th...|2016-10-15|     news|\n",
      "+---------+--------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.load('../data/oct_2016_news_comment.parquet')\n",
    "data = data.sample(False, 0.1, 35).cache()\n",
    "print(data.count())\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'loving', u'person', u'world', u'wondering']\n",
      "[u'loving', u'person', u'world', u'wonder']\n"
     ]
    }
   ],
   "source": [
    "def text_preprocessing(txt, stop_words, pos_tagging=False):\n",
    "    '''\n",
    "    Take care of doing all the text preprocessing for LDA\n",
    "    Only works on english ASCII content. (works on content with accent or such, but filter them out)\n",
    "    '''\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        from nltk.corpus import wordnet\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            #this is the default behaviour for lemmatize. \n",
    "            return wordnet.NOUN\n",
    "    \n",
    "    def remove_https(s):\n",
    "        return re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', s, flags=re.UNICODE)\n",
    "    \n",
    "    #keeping only elements relevant to written speech.\n",
    "    def keep_only_letters(s):\n",
    "        return re.sub('[^a-zA-Z \\']+', '', s, flags=re.UNICODE)\n",
    "    \n",
    "    #remove mark of genitif from speech (i.e. \"'s\" associated to nouns)\n",
    "    def remove_genitive(s): \n",
    "        return re.sub('(\\'s)+', '', s, flags=re.UNICODE)\n",
    "    \n",
    "    def clean_pipeline(s): \n",
    "        return remove_genitive(keep_only_letters(remove_https(s)))\n",
    "    \n",
    "    #tokenizing the texts (removing line break, space and capitalization)\n",
    "    token_comm = re.split(\" \", clean_pipeline(txt).strip().lower())\n",
    "    \n",
    "    #to avoid empty token (caused by multiple spaces in the tokenization)\n",
    "    token_comm = [t for t in token_comm if len(t) > 0]\n",
    "    \n",
    "    if pos_tagging:\n",
    "        token_comm = pos_tag(token_comm)\n",
    "    else:\n",
    "        token_comm = zip(token_comm, [None]*len(token_comm))\n",
    "        \n",
    "    #removing all words of three letters or less\n",
    "    bigger_w = [x for x in token_comm if len(x[0]) > 3]\n",
    "\n",
    "    #removing stop_words\n",
    "    wout_sw_w = [x for x in bigger_w if x[0] not in stop_words]\n",
    "    \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import wordnet\n",
    "    if pos_tagging:\n",
    "    #get lemma of each word, then return result\n",
    "        return [WordNetLemmatizer().lemmatize(word, get_wordnet_pos(tag)) for word, tag in wout_sw_w]\n",
    "    else:\n",
    "        #get lemma of each word, then return result\n",
    "        return [WordNetLemmatizer().lemmatize(word) for word, _ in wout_sw_w]\n",
    "\n",
    "\n",
    "def condense_comm_and_preprocessing(dataset, stop_words, use_pos_tagging=False):\n",
    "    '''\n",
    "    Function whose purpose is to condensate all comments\n",
    "    of one post (identified by link_id) into one array per post \n",
    "    in the resulting dataframe. Transform the data into a format\n",
    "    that is useful to perform LDA on. (considering documents being comment content from post)\n",
    "    Also apply preprocessing on the textual data. \n",
    "    '''\n",
    "    #keeping only what matters\n",
    "    dataset = dataset.select('link_id', 'body', 'created')\n",
    "    \n",
    "    #function to use with aggregateByKey\n",
    "    def zeroValue():\n",
    "        return ([], datetime.date(year=3016, month=12, day=30))\n",
    "    def combFun(l, r):\n",
    "        return (l[0] + r[0], min(l[1], r[1]))\n",
    "    def seqFun(prev, curr):\n",
    "        return (prev[0] + curr[0], prev[1] if prev[1] < curr[1] else curr[1])\n",
    "\n",
    "    #removing post that have been deleted/removed (thus hold no more information)\n",
    "    filtered = dataset.filter(dataset.body != '[removed]').filter(dataset.body != '[deleted]').filter(dataset.body != '')\n",
    "\n",
    "    #removing short comments\n",
    "    big_comm = filtered.rdd.filter(lambda r: len(r[1]) > 50)\n",
    "\n",
    "    #applying preprocessing at the text level, and filtering post with empty tokenization\n",
    "    filtered_rdd = big_comm.map(lambda r: (r[0], (text_preprocessing(r[1],stop_words, use_pos_tagging), r[2]))).filter(lambda r: r[1][0])\n",
    "\n",
    "    #the consequence of the aggregateByKey and zipWithUniqueId makes it that we have tuples of tuples we need to flatten.\n",
    "    post_and_list_token = filtered_rdd.aggregateByKey(zeroValue(), seqFun, combFun).zipWithUniqueId().map(lambda r: (r[0][0], r[0][1][0], r[0][1][1], r[1]))\n",
    "\n",
    "    return post_and_list_token.toDF().selectExpr(\"_1 as post_id\", \"_2 as text\",\"_3 as created\", \"_4 as uid\")\n",
    "\n",
    "\n",
    "\n",
    "def perform_lda(documents, n_topics, n_words, beta, tokens_col):\n",
    "    '''\n",
    "    will perform LDA on a list of documents (== list of token)\n",
    "    assume that documents is a DataFrame with a column of unique id (uid).\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    cv = CountVectorizer(inputCol=tokens_col, outputCol=\"raw_features\")\n",
    "    cvmodel = cv.fit(documents)\n",
    "    result_cv = cvmodel.transform(documents)\n",
    "    \n",
    "    #we perform an tf-idf (term frequency inverse document frequency), to avoid threads with a lot of words to pollute the topics.\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(result_cv)\n",
    "    result_tfidf = idfModel.transform(result_cv) \n",
    "    \n",
    "    #keeping created for time series purpose. \n",
    "    corpus = result_tfidf.select(\"uid\", \"features\", \"created\")\n",
    "    \n",
    "    #defining and running the lda. Em is the best optimizer performance-wise.\n",
    "    #lda = LDA(k=n_topics, optimizer='em', docConcentration=alphas, topicConcentration=beta)\n",
    "    lda = LDA(k=n_topics, topicConcentration=beta)\n",
    "    \n",
    "    model = lda.fit(corpus)\n",
    "    \n",
    "    #retrieving topics, and the vocabulary constructed by the CountVectorizer\n",
    "    topics = model.describeTopics(maxTermsPerTopic=n_words)\n",
    "    vocab = cvmodel.vocabulary\n",
    "    \n",
    "    #getting topic distribution per document. \n",
    "    topic_distribution = model.transform(corpus)[['topicDistribution', 'created']]\n",
    "    \n",
    "    #the topics are just numerical indices, we need to convert them to words, and associate them to their weights..\n",
    "    topics_with_weights = topics.rdd.map(lambda r: (r[0], ([(vocab[t],w) for t,w in zip(r[1], r[2])]), ' '.join([vocab[t] for t in r[1]]))).toDF().selectExpr(\"_1 as topic_number\", \"_2 as topic_weight\", \"_3 as topic\")\n",
    "    \n",
    "    return topics_with_weights, topic_distribution\n",
    "\n",
    "test = u\"What would be the most loving person in the world ? I am wondering\"\n",
    "print(text_preprocessing(test, en_stop, False))\n",
    "print(text_preprocessing(test, en_stop, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u'requirement failed: LDA topicConcentration must be > 1.0 (or -1 for auto) for EM Optimizer, but was set to 0.001'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b58a497a7f7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcondensed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcondense_comm_and_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtopics_n_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperform_lda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondensed_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics_n_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-557c51ab8751>\u001b[0m in \u001b[0;36mperform_lda\u001b[0;34m(documents, n_topics, n_words, beta, tokens_col)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopicConcentration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'em'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m#retrieving topics, and the vocabulary constructed by the CountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.3.2-bin-hadoop2.7/python/pyspark/ml/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/Applications/spark-2.3.2-bin-hadoop2.7/python/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.3.2-bin-hadoop2.7/python/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.3.2-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.3.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: u'requirement failed: LDA topicConcentration must be > 1.0 (or -1 for auto) for EM Optimizer, but was set to 0.001'"
     ]
    }
   ],
   "source": [
    "def display_topics(topics_n_weight):\n",
    "    tops = topics_n_weight.select('topic').collect()\n",
    "    for i in range(len(tops)):\n",
    "        print(\"T %d: %s\"%(i+1, tops[i][0]))\n",
    "\n",
    "condensed_data = condense_comm_and_preprocessing(data, en_stop)\n",
    "topics_n_weight, topic_distribution = perform_lda(condensed_data, 10, 4, 0.001, 'text')\n",
    "display_topics(topics_n_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "min_alpha = 0.01\n",
    "max_alpha = 0.1\n",
    "n_topics = 10\n",
    "alphas = np.linspace(min_alpha,max_alpha,n_topics)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(10):\\n    curr_max_alpha = min_alpha*float(i)\\n    curr_alphas = [curr_max_alpha]*n_topics\\n    for j in [0.01, 0.05]:\\n        curr_beta = j\\n        print(\"\\nAlpha range : [%.3f - %.3f]\"%(min(curr_alphas), max(curr_alphas)))\\n        print(\"Beta value \"+str(curr_beta))\\n        topics_n_weight, _ = perform_lda(curr_data, n_topics, w_topics, curr_alphas, curr_beta, \\'text\\')\\n        display_topics(topics_n_weight)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_alpha = 0.01\n",
    "max_alpha = 0.1\n",
    "\n",
    "base_beta = 0.01\n",
    "\n",
    "n_topics = 10\n",
    "w_topics = 4\n",
    "        \n",
    "#condensed_data = condense_comm_and_preprocessing(data, en_stop)\n",
    "\n",
    "'''\n",
    "for i in range(10):\n",
    "    curr_max_alpha = min_alpha*float(i)\n",
    "    curr_alphas = [curr_max_alpha]*n_topics\n",
    "    for j in [0.01, 0.05]:\n",
    "        curr_beta = j\n",
    "        print(\"\\nAlpha range : [%.3f - %.3f]\"%(min(curr_alphas), max(curr_alphas)))\n",
    "        print(\"Beta value \"+str(curr_beta))\n",
    "        topics_n_weight, _ = perform_lda(curr_data, n_topics, w_topics, curr_alphas, curr_beta, 'text')\n",
    "        display_topics(topics_n_weight)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01,\n",
       " 0.019000000000000003,\n",
       " 0.028000000000000004,\n",
       " 0.037000000000000005,\n",
       " 0.046000000000000006,\n",
       " 0.05500000000000001,\n",
       " 0.064,\n",
       " 0.07300000000000001,\n",
       " 0.08200000000000002,\n",
       " 0.09100000000000003,\n",
       " 0.1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linspace(mi, ma, n):\n",
    "    res = []\n",
    "    idx = mi\n",
    "    incr = float(ma-mi)/n\n",
    "    while idx < ma:\n",
    "        res.append(idx)\n",
    "        idx += incr\n",
    "    res.append(ma)\n",
    "    return res\n",
    "\n",
    "alpha_min =0.01\n",
    "alpha_max =0.1\n",
    "n_topics = 10\n",
    "#best values for alphas are n_topics value range [0.01-0.1] or [0.005-0.05]\n",
    "linspace(alpha_min, alpha_max, n_topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
