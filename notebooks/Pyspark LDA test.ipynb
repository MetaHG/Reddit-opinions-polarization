{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "en_stop = set(stopwords.words('english'))\n",
    "en_lemmatizer = WordNetLemmatizer()\n",
    "sc = spark.sparkContext\n",
    "sc.broadcast(en_stop)\n",
    "sc.broadcast(en_lemmatizer)\n",
    "\n",
    "import datetime\n",
    "import re as re\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.mllib.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "+---------+--------------------+----------+---------+\n",
      "|  link_id|                body|   created|subreddit|\n",
      "+---------+--------------------+----------+---------+\n",
      "|t3_57m54q|I've lost people ...|2016-10-15|     news|\n",
      "|t3_57m54q|We need better a ...|2016-10-15|     news|\n",
      "|t3_5aiyva|          Same in US|2016-11-01|     news|\n",
      "|t3_5ah1kd|&gt; they just ta...|2016-11-01|     news|\n",
      "|t3_57m54q|The lawsuit of a ...|2016-10-15|     news|\n",
      "|t3_57xeeg|In my state at le...|2016-10-18|     news|\n",
      "|t3_57zmi3|Thanks for saying...|2016-10-17|     news|\n",
      "|t3_5aiyva|The fuck up here ...|2016-11-01|     news|\n",
      "|t3_5bg6hy|           [removed]|2016-11-06|     news|\n",
      "|t3_59tp8q|The people who bu...|2016-10-28|     news|\n",
      "|t3_59tp8q|           [removed]|2016-10-28|     news|\n",
      "|t3_57wi3j|Are you for real?...|2016-10-17|     news|\n",
      "|t3_572621|You'd rather kill...|2016-10-12|     news|\n",
      "|t3_56choj|&gt;If no, then y...|2016-10-08|     news|\n",
      "|t3_56f95x|I don't remember ...|2016-10-08|     news|\n",
      "|t3_57tzgl|           [removed]|2016-10-17|     news|\n",
      "|t3_56b7zl|My point is:  why...|2016-10-08|     news|\n",
      "|t3_5af069|           [removed]|2016-10-31|     news|\n",
      "|t3_573p94|It is different t...|2016-10-12|     news|\n",
      "|t3_56xyb2|Sounds like they ...|2016-10-12|     news|\n",
      "+---------+--------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.load('../data/oct_2016_news_comment.parquet')\n",
    "data = data.sample(False, 0.0001, 35).cache()\n",
    "print(data.count())\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_comment_preprocessing(txt, stop_words, lemmatizer):\n",
    "    '''\n",
    "    Take care of doing all the text preprocessing for LDA at the comment level\n",
    "    Only works on english ASCII content. (works on content with accent or such, but filter them out)\n",
    "    '''\n",
    "    #necessary apparently to avoid this error \"pickle.PicklingError: args[0] from newobj args has the wrong class\"\n",
    "    from nltk.corpus import wordnet\n",
    "    \n",
    "    #keeping only elements relevant to written speech.\n",
    "    keep_only_letters = lambda s: re.sub('[^a-zA-Z \\']+', '', s)\n",
    "    \n",
    "    #remove mark of genitif from speech (i.e. \"'s\" associated to nouns)\n",
    "    remove_genitive = lambda s: re.sub('(\\'s)+', '', s)\n",
    "    \n",
    "    #tokenizing the texts (removing line break, space and capitalization)\n",
    "    token_comm = re.split(\" \", remove_genitive(keep_only_letters(txt)).strip().lower())\n",
    "    \n",
    "    #removing all words of three letters or less\n",
    "    bigger_w = [x for x in token_comm if len(x) > 3] \n",
    "    \n",
    "    #removing stop_words\n",
    "    wout_sw_w = [x for x in bigger_w if x not in stop_words]\n",
    "    \n",
    "    #get lemma of each word, then return result\n",
    "    return [lemmatizer.lemmatize(word) for word in wout_sw_w]\n",
    "\n",
    "\n",
    "def dataset_cleaning_and_preprocessing(data, stop_words, lemmatizer):\n",
    "    '''\n",
    "    take a pyspark dataframe as input,\n",
    "    transform it into a RDD, and filter all \n",
    "    empty comments ([removed], [deleted], or just empty text)\n",
    "    and apply the preprocessing for lda.\n",
    "    '''\n",
    "    #keeping only what matters\n",
    "    dataset = data.select('link_id', 'body', 'created')\n",
    "    \n",
    "    #removing post that have been deleted/removed (thus hold no more information)\n",
    "    filtered = dataset.filter(dataset.body != '[removed]').filter(dataset.body != '[deleted]').filter(dataset.body != '')\n",
    "    \n",
    "    #applying comment preprocessing for LDA\n",
    "    preprocessed = filtered.rdd.map(lambda r: (r[0], reddit_comment_preprocessing(r[1], stop_words, lemmatizer), r[2]))\n",
    "    \n",
    "    #we return only posts which have actual textual comments. (hence the filter with the second row element (pythonic way of testing non emptiness of list))\n",
    "    preprocessed_filtered = preprocessed.filter(lambda r : r[1])\n",
    "    \n",
    "    return preprocessed.map(lambda r: (r[0], (r[1], r[2])))\n",
    "\n",
    "\n",
    "def perform_lda(documents, n_topics, n_words):\n",
    "    '''\n",
    "    will perform LDA on a list of documents (== list of token)\n",
    "    assume that documents is a RDD.\n",
    "    '''\n",
    "    documents_df = documents.zipWithUniqueId().toDF().selectExpr(\"_1 as tokens\", \"_2 as uid\")\n",
    "    \n",
    "    #TODO : fine tune parameters\n",
    "    cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\")\n",
    "    cvmodel = cv.fit(documents_df)\n",
    "    result_cv = cvmodel.transform(documents_df)\n",
    "    \n",
    "    #we perform an tf-idf (term frequency inverse document frequency), to avoid comments with a lot of words to pollute the topics.\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(result_cv)\n",
    "    result_tfidf = idfModel.transform(result_cv) \n",
    "    \n",
    "    corpus = result_tfidf.select(\"uid\", \"features\").rdd.map(lambda r: [r[0],Vectors.fromML(r[1])])\n",
    "    model = LDA.train(corpus, k=n_topics, maxIterations=10)\n",
    "\n",
    "    topics = model.describeTopics(maxTermsPerTopic=n_words)\n",
    "    \n",
    "    vocab = cvmodel.vocabulary\n",
    "    \n",
    "    terms = topics[0]\n",
    "    \n",
    "    #topic[0] represents the words, topic[1] their weights\n",
    "    return [[(vocab[topic[0][n]], topic[1][n]) for n in range(len(topic[0]))] for _, topic in enumerate(topics)]\n",
    "\n",
    "\n",
    "def lda_and_min_date(in_rdd, n_topics, n_words):\n",
    "    tok_date = in_rdd.values()\n",
    "    min_date = tok_date.values().min()\n",
    "    topics = perform_lda(tok_date.keys(), n_topics, n_words)\n",
    "    return (topics, min_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_preprocessed = dataset_cleaning_and_preprocessing(data, en_stop, en_lemmatizer)\n",
    "individual_keys = cleaned_preprocessed.keys().distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_res_sample = [lda_and_min_date(cleaned_preprocessed.filter(lambda r: r[0] == post_id), 1, 3) for post_id in individual_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "res_df = sc.parallelize(lda_res_sample).toDF().selectExpr(\"_1 as topic\", \"_2 as date\")\n",
    "res_df.write.mode('overwrite').parquet('2016_news_lda_sample_3_WornetStem.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               topic|      date|\n",
      "+--------------------+----------+\n",
      "|[[[america, 0.133...|2016-10-23|\n",
      "|                [[]]|2016-10-08|\n",
      "|                [[]]|2016-10-25|\n",
      "|                [[]]|2016-10-24|\n",
      "|                [[]]|2016-10-24|\n",
      "|                [[]]|2016-10-11|\n",
      "|                [[]]|2016-10-23|\n",
      "|                [[]]|2016-10-26|\n",
      "|                [[]]|2016-10-14|\n",
      "|                [[]]|2016-10-14|\n",
      "|                [[]]|2016-10-10|\n",
      "|[[[eleven, 0.1666...|2016-11-01|\n",
      "|                [[]]|2016-10-28|\n",
      "|                [[]]|2016-10-12|\n",
      "|                [[]]|2016-10-12|\n",
      "|                [[]]|2016-10-12|\n",
      "|                [[]]|2016-10-23|\n",
      "|                [[]]|2016-10-25|\n",
      "|                [[]]|2016-11-01|\n",
      "|                [[]]|2016-10-12|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_data = spark.read.load('2016_news_lda_sample_3.parquet')\n",
    "res_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               topic|      date|\n",
      "+--------------------+----------+\n",
      "|[[[america, 0.133...|2016-10-23|\n",
      "|                [[]]|2016-10-08|\n",
      "|                [[]]|2016-10-25|\n",
      "|                [[]]|2016-10-24|\n",
      "|                [[]]|2016-10-24|\n",
      "|                [[]]|2016-10-11|\n",
      "|                [[]]|2016-10-23|\n",
      "|                [[]]|2016-10-26|\n",
      "|                [[]]|2016-10-14|\n",
      "|                [[]]|2016-10-14|\n",
      "|                [[]]|2016-10-10|\n",
      "|[[[eleven, 0.1666...|2016-11-01|\n",
      "|                [[]]|2016-10-28|\n",
      "|                [[]]|2016-10-12|\n",
      "|                [[]]|2016-10-12|\n",
      "|                [[]]|2016-10-12|\n",
      "|                [[]]|2016-10-23|\n",
      "|                [[]]|2016-10-25|\n",
      "|                [[]]|2016-11-01|\n",
      "|                [[]]|2016-10-12|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_data_2 = spark.read.load('2016_news_lda_sample_3_WornetStem.parquet')\n",
    "res_data_2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
