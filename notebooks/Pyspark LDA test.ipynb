{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources : \n",
    "[General data science article presenting the method](https://medium.com/@connectwithghosh/topic-modelling-with-latent-dirichlet-allocation-lda-in-pyspark-2cb3ebd5678e)\n",
    "\n",
    "[Issue of an user to do LDA on SO](https://stackoverflow.com/questions/42051184/latent-dirichlet-allocation-lda-in-spark#)\n",
    "\n",
    "[Gist of a LDA implementation](https://gist.github.com/Bergvca/a59b127afe46c1c1c479)\n",
    "\n",
    "[SO question about retrieving topic distribution](https://stackoverflow.com/questions/33072449/extract-document-topic-matrix-from-pyspark-lda-model/41515070)\n",
    "\n",
    "[Slides from presentation about recommended parameters for LDA](http://www.phusewiki.org/wiki/images/c/c9/Weizhong_Presentation_CDER_Nov_9th.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "#other words that need to be removed in order to avoid pollution on the topic. (experimental findings on news subreddit)\n",
    "aux_stop_words = ['people', 'would', 'like']\n",
    "\n",
    "en_stop = set(stopwords.words('english')+aux_stop_words)\n",
    "sc = spark.sparkContext\n",
    "sc.broadcast(en_stop)\n",
    "\n",
    "import datetime\n",
    "import re as re\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52354\n",
      "+---------+--------------------+----------+---------+\n",
      "|  link_id|                body|   created|subreddit|\n",
      "+---------+--------------------+----------+---------+\n",
      "|t3_57m54q|Good luck to you ...|2016-10-15|     news|\n",
      "|t3_57m54q|Because he's not ...|2016-10-15|     news|\n",
      "|t3_57m54q|I do not know man...|2016-10-15|     news|\n",
      "|t3_57gfbu|Did the man have ...|2016-10-15|     news|\n",
      "|t3_57m54q|Good ol college l...|2016-10-15|     news|\n",
      "|t3_57m81k|&gt; Noone deserv...|2016-10-15|     news|\n",
      "|t3_57m54q|Well, that's a pr...|2016-10-15|     news|\n",
      "|t3_57m54q|A car cant be con...|2016-10-15|     news|\n",
      "|t3_57m54q|No. I own a singl...|2016-10-15|     news|\n",
      "|t3_57m54q|It's a problem th...|2016-10-15|     news|\n",
      "|t3_57m54q|           [removed]|2016-10-15|     news|\n",
      "|t3_57m54q|Guns have a huge ...|2016-10-15|     news|\n",
      "|t3_57gi47|seriously, a word...|2016-10-15|     news|\n",
      "|t3_57m54q|&gt; I dont see w...|2016-10-15|     news|\n",
      "|t3_57m54q|Unless the name o...|2016-10-15|     news|\n",
      "|t3_57ljf8|You realize most ...|2016-10-15|     news|\n",
      "|t3_57m54q|So if I stab you,...|2016-10-15|     news|\n",
      "|t3_57m54q|He would more lik...|2016-10-15|     news|\n",
      "|t3_57m54q|Why do people kee...|2016-10-15|     news|\n",
      "|t3_57hn6w|Well it's more th...|2016-10-15|     news|\n",
      "+---------+--------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.load('../data/oct_2016_news_comment.parquet')\n",
    "data = data.sample(False, 0.1, 35).cache()\n",
    "print(data.count())\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loving', 'person', 'world', 'wondering']\n",
      "['loving', 'person', 'world', 'wonder']\n"
     ]
    }
   ],
   "source": [
    "def text_preprocessing(txt, stop_words, pos_tagging=False):\n",
    "    '''\n",
    "    Take care of doing all the text preprocessing for LDA\n",
    "    Only works on english ASCII content. (works on content with accent or such, but filter them out)\n",
    "    '''\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        from nltk.corpus import wordnet\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            #this is the default behaviour for lemmatize. \n",
    "            return wordnet.NOUN\n",
    "    \n",
    "    remove_https = lambda s: re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', s)\n",
    "    \n",
    "    #keeping only elements relevant to written speech.\n",
    "    keep_only_letters = lambda s: re.sub('[^a-zA-Z \\']+', '', s)\n",
    "    \n",
    "    #remove mark of genitif from speech (i.e. \"'s\" associated to nouns)\n",
    "    remove_genitive = lambda s: re.sub('(\\'s)+', '', s)\n",
    "    \n",
    "    clean_pipeline = lambda s: remove_genitive(keep_only_letters(remove_https(s)))\n",
    "    \n",
    "    #tokenizing the texts (removing line break, space and capitalization)\n",
    "    token_comm = re.split(\" \", clean_pipeline(txt).strip().lower())\n",
    "    \n",
    "    #to avoid empty token (caused by multiple spaces in the tokenization)\n",
    "    token_comm = [t for t in token_comm if len(t) > 0]\n",
    "    \n",
    "    if pos_tagging:\n",
    "        token_comm = pos_tag(token_comm)\n",
    "    else:\n",
    "        token_comm = zip(token_comm, [None]*len(token_comm))\n",
    "        \n",
    "    #removing all words of three letters or less\n",
    "    bigger_w = [x for x in token_comm if len(x[0]) > 3]\n",
    "\n",
    "    #removing stop_words\n",
    "    wout_sw_w = [x for x in bigger_w if x[0] not in stop_words]\n",
    "    \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import wordnet\n",
    "    if pos_tagging:\n",
    "    #get lemma of each word, then return result\n",
    "        return [WordNetLemmatizer().lemmatize(word, get_wordnet_pos(tag)) for word, tag in wout_sw_w]\n",
    "    else:\n",
    "        #get lemma of each word, then return result\n",
    "        return [WordNetLemmatizer().lemmatize(word) for word, _ in wout_sw_w]\n",
    "\n",
    "\n",
    "def condense_comm_and_preprocessing(dataset, stop_words):\n",
    "    '''\n",
    "    Function whose purpose is to condensate all comments\n",
    "    of one post (identified by link_id) into one array per post \n",
    "    in the resulting dataframe. Transform the data into a format\n",
    "    that is useful to perform LDA on. (considering documents being comment content from post)\n",
    "    Also apply preprocessing on the textual data. \n",
    "    '''\n",
    "    #keeping only what matters\n",
    "    dataset = dataset.select('link_id', 'body', 'created')\n",
    "    \n",
    "    zeroValue = (list(), datetime.date(year=3016, month=12, day=30))\n",
    "    combFun = lambda l, r: (l[0] + r[0], min(l[1], r[1]))\n",
    "    seqFun = lambda prev, curr: (prev[0] + curr[0], prev[1] if prev[1] < curr[1] else curr[1])\n",
    "    \n",
    "    #removing post that have been deleted/removed (thus hold no more information)\n",
    "    filtered = dataset.filter(dataset.body != '[removed]').filter(dataset.body != '[deleted]').filter(dataset.body != '')\n",
    "    \n",
    "    #applying preprocessing at the text level, and filtering post with empty tokenization\n",
    "    filtered_rdd = filtered.rdd.map(lambda r: (r[0], (text_preprocessing(r[1],stop_words, True), r[2]))).filter(lambda r: r[1][0])\n",
    "    \n",
    "    #the consequence of the aggregateByKey and zipWithUniqueId makes it that we have tuples of tuples we need to flatten.\n",
    "    post_and_list_token = filtered_rdd.aggregateByKey(zeroValue, seqFun, combFun).map(lambda r: (r[0], r[1][0], r[1][1]))\n",
    "    \n",
    "    withUid = post_and_list_token.zipWithUniqueId().map(lambda r: (r[0][0], r[0][1], r[0][2], r[1]))\n",
    "    \n",
    "    return withUid.toDF().selectExpr(\"_1 as post_id\", \"_2 as text\",\"_3 as created\", \"_4 as uid\")\n",
    "\n",
    "def perform_lda(documents, n_topics, n_words, alphas, beta, tokens_col):\n",
    "    '''\n",
    "    will perform LDA on a list of documents (== list of token)\n",
    "    assume that documents is a DataFrame with a column of unique id (uid).\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    cv = CountVectorizer(inputCol=tokens_col, outputCol=\"raw_features\")\n",
    "    cvmodel = cv.fit(documents)\n",
    "    result_cv = cvmodel.transform(documents)\n",
    "    \n",
    "    #we perform an tf-idf (term frequency inverse document frequency), to avoid threads with a lot of words to pollute the topics.\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(result_cv)\n",
    "    result_tfidf = idfModel.transform(result_cv) \n",
    "    \n",
    "    #keeping created for time series purpose. \n",
    "    corpus = result_tfidf.select(\"uid\", \"features\", \"created\")\n",
    "    \n",
    "    #defining and running the lda. Em is the best optimizer performance-wise.\n",
    "    #lda = LDA(k=n_topics, optimizer='em', docConcentration=alphas, topicConcentration=beta)\n",
    "    lda = LDA(k=n_topics, docConcentration=alphas, topicConcentration=beta)\n",
    "    \n",
    "    model = lda.fit(corpus)\n",
    "    \n",
    "    #retrieving topics, and the vocabulary constructed by the CountVectorizer\n",
    "    topics = model.describeTopics(maxTermsPerTopic=n_words)\n",
    "    vocab = cvmodel.vocabulary\n",
    "    \n",
    "    #getting topic distribution per document. \n",
    "    topic_distribution = model.transform(corpus)[['topicDistribution', 'created']]\n",
    "    \n",
    "    #the topics are just numerical indices, we need to convert them to words, and associate them to their weights..\n",
    "    topics_with_weights = topics.rdd.map(lambda r: (r[0], ([(vocab[t],w) for t,w in zip(r[1], r[2])]), ' '.join([vocab[t] for t in r[1]]))).toDF().selectExpr(\"_1 as topic_number\", \"_2 as topic_weight\", \"_3 as topic\")\n",
    "    \n",
    "    return topics_with_weights, topic_distribution\n",
    "\n",
    "test = \"What would be the most loving person in the world ? I am wondering\"\n",
    "print(text_preprocessing(test, en_stop, False))\n",
    "print(text_preprocessing(test, en_stop, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T 1: pipeline walmart reactor feral\n",
      "T 2: cigar kratom breathalyzer sobriety\n",
      "T 3: yoga netflix feral pant\n",
      "T 4: recycle cup compost recycled\n",
      "T 5: make think know say\n",
      "T 6: white sikh racist black\n",
      "T 7: rape vine tuggle bathroom\n",
      "T 8: atampt tragic marshal thailand\n",
      "T 9: assange rape kevin solitary\n",
      "T 10: samsung phone battery iphone\n"
     ]
    }
   ],
   "source": [
    "def display_topics(topics_n_weight):\n",
    "    tops = topics_n_weight.select('topic').collect()\n",
    "    for i in range(len(tops)):\n",
    "        print(\"T %d: %s\"%(i+1, tops[i][0]))\n",
    "\n",
    "condensed_data = condense_comm_and_preprocessing(data, en_stop)\n",
    "topics_n_weight, topic_distribution = perform_lda(condensed_data, 10, 4, [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1], 0.001, 'text')\n",
    "display_topics(topics_n_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "min_alpha = 0.01\n",
    "max_alpha = 0.1\n",
    "n_topics = 10\n",
    "alphas = np.linspace(min_alpha,max_alpha,n_topics)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha range : [0.000 - 0.000]\n",
      "Beta value 0.01\n",
      "T 1: jury poop chicken juror\n",
      "T 2: pipeline reactor tesla yoga\n",
      "T 3: kratom netflix duterte thailand\n",
      "T 4: woman child rape say\n",
      "T 5: nobel dylan prize award\n",
      "T 6: make think go even\n",
      "T 7: recycle cup compost feral\n",
      "T 8: marshal launcher lithuania cameron\n",
      "T 9: vine haiti dominican hallucination\n",
      "T 10: sketch adobe figma designer\n",
      "\n",
      "Alpha range : [0.000 - 0.000]\n",
      "Beta value 0.05\n",
      "T 1: recycle rape woman cup\n",
      "T 2: kratom marshal murder creeper\n",
      "T 3: pipeline assange leak land\n",
      "T 4: kit muslim pistol thailand\n",
      "T 5: samsung phone battery iphone\n",
      "T 6: make think know say\n",
      "T 7: tesla cake reactor breh\n",
      "T 8: cigar poop zombie venison\n",
      "T 9: feral sketch marriage marry\n",
      "T 10: zimmerman jury yoga vine\n",
      "\n",
      "Alpha range : [0.010 - 0.010]\n",
      "Beta value 0.01\n",
      "T 1: recycle cup compost recycled\n",
      "T 2: vine sketch adobe figma\n",
      "T 3: pipeline yoga reactor land\n",
      "T 4: make think say know\n",
      "T 5: religion sikh assange muslim\n",
      "T 6: insurance cost company hospital\n",
      "T 7: tesla marshal mason gigafactory\n",
      "T 8: atampt feral cigar haiti\n",
      "T 9: cub cake duterte clown\n",
      "T 10: reactor radiation nuclear quota\n",
      "\n",
      "Alpha range : [0.010 - 0.010]\n",
      "Beta value 0.05\n",
      "T 1: recycle cup compost recycled\n",
      "T 2: saudi philippine ca zombie\n",
      "T 3: yoga duterte cigar woman\n",
      "T 4: shoot gun police rifle\n",
      "T 5: make think thing know\n",
      "T 6: muslim white sikh black\n",
      "T 7: atampt tesla vine warner\n",
      "T 8: samsung iphone sketch visa\n",
      "T 9: thailand urinal bitcoin lithuania\n",
      "T 10: feral pipeline reactor radiation\n",
      "\n",
      "Alpha range : [0.020 - 0.020]\n",
      "Beta value 0.01\n",
      "T 1: recycle cup compost recycled\n",
      "T 2: kratom unnecessarily untested cherokee\n",
      "T 3: jury juror cigar poop\n",
      "T 4: tesla tort shoplifter patent\n",
      "T 5: trump religion hillary make\n",
      "T 6: make think go even\n",
      "T 7: feral reactor pipeline police\n",
      "T 8: vine sketch marshal adobe\n",
      "T 9: rape woman nobel child\n",
      "T 10: atampt encrypt tragic fiber\n",
      "\n",
      "Alpha range : [0.020 - 0.020]\n",
      "Beta value 0.05\n",
      "T 1: recycle cup feral compost\n",
      "T 2: pipeline reactor kit nuclear\n",
      "T 3: samsung battery nobel phone\n",
      "T 4: jury sikh shoot rape\n",
      "T 5: yoga cigar vine rape\n",
      "T 6: make think know say\n",
      "T 7: tesla netflix crevasse warner\n",
      "T 8: atampt kratom yahoo marshal\n",
      "T 9: assange breathalyzer cake sketch\n",
      "T 10: marriage kevin marry blockade\n",
      "\n",
      "Alpha range : [0.030 - 0.030]\n",
      "Beta value 0.01\n",
      "T 1: woman child rape say\n",
      "T 2: yoga feral samsung cigar\n",
      "T 3: kratom sketch marshal adobe\n",
      "T 4: make police know think\n",
      "T 5: recycle cup compost recycled\n",
      "T 6: insurance cost company make\n",
      "T 7: proofreader cigar flood floyd\n",
      "T 8: atampt duterte encrypt smithsonian\n",
      "T 9: cub vine kit untested\n",
      "T 10: mason muir bitcoin meteor\n",
      "\n",
      "Alpha range : [0.030 - 0.030]\n",
      "Beta value 0.05\n",
      "T 1: white trump think black\n",
      "T 2: dashcam police sobriety cigar\n",
      "T 3: juror jury tragic bundys\n",
      "T 4: make insurance think know\n",
      "T 5: recycle compost yoga recycled\n",
      "T 6: solar company phone power\n",
      "T 7: vine cake bakery rape\n",
      "T 8: feral kratom crevasse proofreader\n",
      "T 9: marshal tuggle kit hallucination\n",
      "T 10: recycle compost recycled flood\n",
      "\n",
      "Alpha range : [0.040 - 0.040]\n",
      "Beta value 0.01\n",
      "T 1: samsung phone battery iphone\n",
      "T 2: yoga cigar vine haiti\n",
      "T 3: breathalyzer opiate cup thailand\n",
      "T 4: feral reactor kit chicken\n",
      "T 5: sikh muslim dylan song\n",
      "T 6: make think know say\n",
      "T 7: tesla cake diversity police\n",
      "T 8: atampt kratom sketch encrypt\n",
      "T 9: recycle pipeline cup compost\n",
      "T 10: yahoo ca wayne gunman\n",
      "\n",
      "Alpha range : [0.040 - 0.040]\n",
      "Beta value 0.05\n",
      "T 1: recycle cup compost recycled\n",
      "T 2: yoga vine poop pant\n",
      "T 3: reactor radiation breh nuclear\n",
      "T 4: jury shoot rifle feral\n",
      "T 5: make think say know\n",
      "T 6: white black pipeline racist\n",
      "T 7: cigar tesla haiti dominican\n",
      "T 8: sketch cake adobe figma\n",
      "T 9: rape woman tragic consent\n",
      "T 10: samsung battery phone iphone\n",
      "\n",
      "Alpha range : [0.050 - 0.050]\n",
      "Beta value 0.01\n",
      "T 1: samsung recycle cup phone\n",
      "T 2: feral visa chicken tort\n",
      "T 3: jury assange ecuador shoot\n",
      "T 4: cigar kit missile test\n",
      "T 5: make think know say\n",
      "T 6: sikh white terrorist racist\n",
      "T 7: tesla cake zombie bakery\n",
      "T 8: yoga kratom pant ca\n",
      "T 9: atampt vine warner accuser\n",
      "T 10: sketch poop temperature adobe\n",
      "\n",
      "Alpha range : [0.050 - 0.050]\n",
      "Beta value 0.05\n",
      "T 1: cigar cake dylan rape\n",
      "T 2: samsung recycle battery phone\n",
      "T 3: cub marshal iceland smithsonian\n",
      "T 4: jury juror breathalyzer verdict\n",
      "T 5: make think know say\n",
      "T 6: sikh feral white marriage\n",
      "T 7: atampt vine warner cigar\n",
      "T 8: sketch adobe figma designer\n",
      "T 9: assange kevin veganism vegan\n",
      "T 10: paterno sandusky mcqueary murder\n",
      "\n",
      "Alpha range : [0.060 - 0.060]\n",
      "Beta value 0.01\n",
      "T 1: woman child rape say\n",
      "T 2: yoga feral samsung tesla\n",
      "T 3: kratom sketch adobe marshal\n",
      "T 4: police make know gun\n",
      "T 5: recycle cup compost recycled\n",
      "T 6: insurance cost make company\n",
      "T 7: proofreader cigar flood floyd\n",
      "T 8: atampt duterte encrypt smithsonian\n",
      "T 9: vine cub kit thailand\n",
      "T 10: mason muir meteor cliven\n",
      "\n",
      "Alpha range : [0.060 - 0.060]\n",
      "Beta value 0.05\n",
      "T 1: recycle cup compost recycled\n",
      "T 2: kratom smithsonian unnecessarily untested\n",
      "T 3: jury juror cigar poop\n",
      "T 4: tesla tort shoplifter murder\n",
      "T 5: trump religion hillary make\n",
      "T 6: make think go even\n",
      "T 7: reactor pipeline feral police\n",
      "T 8: vine sketch marshal adobe\n",
      "T 9: rape woman nobel child\n",
      "T 10: atampt encrypt tragic fiber\n",
      "\n",
      "Alpha range : [0.070 - 0.070]\n",
      "Beta value 0.01\n",
      "T 1: rape woman feminist consent\n",
      "T 2: vine walmart lithuania proofreader\n",
      "T 3: atampt warner smithsonian urinal\n",
      "T 4: feral jury juror verdict\n",
      "T 5: make think know say\n",
      "T 6: sikh white black dylan\n",
      "T 7: pipeline reactor tesla nuclear\n",
      "T 8: chicken sketch tort adobe\n",
      "T 9: cigar haiti ca tuggle\n",
      "T 10: recycle cup compost recycled\n",
      "\n",
      "Alpha range : [0.070 - 0.070]\n",
      "Beta value 0.05\n",
      "T 1: vine sikh tuggle sikhs\n",
      "T 2: cigar sketch adobe dashcam\n",
      "T 3: yoga trans kratom woman\n",
      "T 4: recycle cup compost jury\n",
      "T 5: healthcare insurance bill doctor\n",
      "T 6: make think know say\n",
      "T 7: feral tesla cake tort\n",
      "T 8: poop aryan wristmaybe muir\n",
      "T 9: kit crevasse untested rape\n",
      "T 10: atampt encrypt assange encryption\n",
      "\n",
      "Alpha range : [0.080 - 0.080]\n",
      "Beta value 0.01\n",
      "T 1: cake woman poop discrimination\n",
      "T 2: samsung battery phone pipeline\n",
      "T 3: sketch chicken tort patent\n",
      "T 4: recycle sikh cup compost\n",
      "T 5: assange ecuador jury wikileaks\n",
      "T 6: make think know say\n",
      "T 7: visa marshal paterno kevin\n",
      "T 8: feral cigar vine zombie\n",
      "T 9: kit ca untested crevasse\n",
      "T 10: launcher murica rev aryan\n",
      "\n",
      "Alpha range : [0.080 - 0.080]\n",
      "Beta value 0.05\n",
      "T 1: tesla kratom jury juror\n",
      "T 2: yoga breathalyzer sobriety pant\n",
      "T 3: vine thailand cake smithsonian\n",
      "T 4: recycle cup compost recycled\n",
      "T 5: make think go know\n",
      "T 6: pipeline reactor nuclear land\n",
      "T 7: cigar haiti dominican crevasse\n",
      "T 8: atampt encrypt encryption fiber\n",
      "T 9: white woman black rape\n",
      "T 10: feral sketch chicken tort\n",
      "\n",
      "Alpha range : [0.090 - 0.090]\n",
      "Beta value 0.01\n",
      "T 1: recycle cup compost recycled\n",
      "T 2: pipeline tesla vine radiation\n",
      "T 3: feral chicken tort breh\n",
      "T 4: ecuador sikh assange military\n",
      "T 5: insurance cost hospital solar\n",
      "T 6: jury yoga cub breathalyzer\n",
      "T 7: cane zombie aryan multifamily\n",
      "T 8: cake paterno bakery crevasse\n",
      "T 9: make think know say\n",
      "T 10: atampt sketch cigar encrypt\n",
      "\n",
      "Alpha range : [0.090 - 0.090]\n",
      "Beta value 0.05\n",
      "T 1: recycle cup compost recycled\n",
      "T 2: pearl creeper fry venison\n",
      "T 3: yoga pant marry marriage\n",
      "T 4: cigar cub tort duterte\n",
      "T 5: make government solar power\n",
      "T 6: think make insurance say\n",
      "T 7: kit vine cake samsung\n",
      "T 8: kratom atampt marshal kevin\n",
      "T 9: feral sketch thailand adobe\n",
      "T 10: mason sikhs dmso wristmaybe\n"
     ]
    }
   ],
   "source": [
    "min_alpha = 0.01\n",
    "max_alpha = 0.1\n",
    "\n",
    "base_beta = 0.01\n",
    "\n",
    "n_topics = 10\n",
    "w_topics = 4\n",
    "        \n",
    "condensed_data = condense_comm_and_preprocessing(data, en_stop)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    curr_max_alpha = min_alpha*float(i)\n",
    "    curr_alphas = [curr_max_alpha]*n_topics\n",
    "    for j in [0.01, 0.05]:\n",
    "        curr_beta = j\n",
    "        print(\"\\nAlpha range : [%.3f - %.3f]\"%(min(curr_alphas), max(curr_alphas)))\n",
    "        print(\"Beta value \"+str(curr_beta))\n",
    "        topics_n_weight, _ = perform_lda(curr_data, n_topics, w_topics, curr_alphas, curr_beta, 'text')\n",
    "        display_topics(topics_n_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(2015, 2016)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(2015,2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = ('2016', topics_n_weight.select('topic').collect())\n",
    "r2 = ('2017', topics_n_weight.select('topic').collect())\n",
    "res = sc.parallelize([r, r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|date|              topics|\n",
      "+----+--------------------+\n",
      "|2016|[[recycle cup com...|\n",
      "|2017|[[recycle cup com...|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.toDF().selectExpr('_1 as date', '_2 as topics').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha range : [0.000 - 0.000]\n",
      "Beta value 0.01\n",
      "T 1: football sport dylan stadium\n",
      "T 2: tuggle vine mail zombie\n",
      "T 3: tort chicken feral kevin\n",
      "T 4: make think say know\n",
      "T 5: rape child priest rap\n",
      "T 6: solar land company pipeline\n",
      "T 7: cigar haiti dominican tragic\n",
      "T 8: breh solitary password proofreader\n",
      "T 9: recycle cup compost recycled\n",
      "T 10: sketch duterte adobe figma\n",
      "\n",
      "Alpha range : [0.000 - 0.000]\n",
      "Beta value 0.05\n",
      "T 1: feral trans yoga woman\n",
      "T 2: recycle cup compost recycled\n",
      "T 3: reactor pipeline cub radiation\n",
      "T 4: kit kratom rape tuggle\n",
      "T 5: make think know say\n",
      "T 6: samsung phone police white\n",
      "T 7: cigar tesla haiti poop\n",
      "T 8: duterte tragic juror jury\n",
      "T 9: trump assange sikh hillary\n",
      "T 10: atampt vine sketch encrypt\n"
     ]
    }
   ],
   "source": [
    "curr_alphas = [0.0]*n_topics\n",
    "for j in [0.01, 0.05]:\n",
    "    curr_beta = j\n",
    "    print(\"\\nAlpha range : [%.3f - %.3f]\"%(min(curr_alphas), max(curr_alphas)))\n",
    "    print(\"Beta value \"+str(curr_beta))\n",
    "    topics_n_weight, _ = perform_lda(curr_data, n_topics, w_topics, curr_alphas, curr_beta, 'text')\n",
    "    display_topics(topics_n_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01,\n",
       " 0.019000000000000003,\n",
       " 0.028000000000000004,\n",
       " 0.037000000000000005,\n",
       " 0.046000000000000006,\n",
       " 0.05500000000000001,\n",
       " 0.064,\n",
       " 0.07300000000000001,\n",
       " 0.08200000000000002,\n",
       " 0.09100000000000003,\n",
       " 0.1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linspace(mi, ma, n):\n",
    "    res = []\n",
    "    idx = mi\n",
    "    incr = float(ma-mi)/n\n",
    "    while idx < ma:\n",
    "        res.append(idx)\n",
    "        idx += incr\n",
    "    res.append(ma)\n",
    "    return res\n",
    "\n",
    "alpha_min =0.01\n",
    "alpha_max =0.1\n",
    "n_topics = 10\n",
    "#best values for alphas are n_topics value range [0.01-0.1] or [0.005-0.05]\n",
    "linspace(alpha_min, alpha_max, n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-459b3de24f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "t = lambda r: print(3); r\n",
    "r(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
