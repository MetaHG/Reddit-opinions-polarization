{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources : \n",
    "[General data science article presenting the method](https://medium.com/@connectwithghosh/topic-modelling-with-latent-dirichlet-allocation-lda-in-pyspark-2cb3ebd5678e)\n",
    "\n",
    "[Issue of an user to do LDA on SO](https://stackoverflow.com/questions/42051184/latent-dirichlet-allocation-lda-in-spark#)\n",
    "\n",
    "[Gist of a LDA implementation](https://gist.github.com/Bergvca/a59b127afe46c1c1c479)\n",
    "\n",
    "[SO question about retrieving topic distribution](https://stackoverflow.com/questions/33072449/extract-document-topic-matrix-from-pyspark-lda-model/41515070)\n",
    "\n",
    "[Slides from presentation about recommended parameters for LDA](http://www.phusewiki.org/wiki/images/c/c9/Weizhong_Presentation_CDER_Nov_9th.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "#other words that need to be removed in order to avoid pollution on the topic. (experimental findings on news subreddit)\n",
    "aux_stop_words = ['people', 'would', 'like', 'img', 'jpg', 'imgjpg', 'botcontact', 'picthis']\n",
    "\n",
    "en_stop = set(stopwords.words('english')+aux_stop_words)\n",
    "sc = spark.sparkContext\n",
    "sc.broadcast(en_stop)\n",
    "\n",
    "import datetime\n",
    "import re as re\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trump = spark.read.load('../data/donald_comments.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(txt, stop_words, pos_tagging=False, use_lemmatizing=False):\n",
    "    '''\n",
    "    Take care of doing all the text preprocessing for LDA\n",
    "    Only works on english ASCII content. (works on content with accent or such, but filter them out)\n",
    "    '''\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            #this is the default behaviour for lemmatize. \n",
    "            return wordnet.NOUN\n",
    "    \n",
    "    #in the case of r/hillaryclinton, removes an annoying omnipresent bot\n",
    "    if 'reddit-bot' in txt:\n",
    "        return []\n",
    "    \n",
    "    def remove_https(s):\n",
    "        return re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', s, flags=re.UNICODE)\n",
    "    \n",
    "    #keeping only elements relevant to written speech.\n",
    "    def keep_only_letters(s):\n",
    "        return re.sub('[^a-zA-Z \\']+', '', s, flags=re.UNICODE)\n",
    "    \n",
    "    #remove mark of genitif from speech (i.e. \"'s\" associated to nouns)\n",
    "    def remove_genitive(s): \n",
    "        return re.sub('(\\'s)+', '', s, flags=re.UNICODE)\n",
    "    \n",
    "    #to avoid things that should be stop words (the're, I'll, wouldn't) to pollute the topics.\n",
    "    def remove_quote(s):\n",
    "        return s.replace('\\'', ' ')\n",
    "    \n",
    "    def clean_pipeline(s): \n",
    "        return remove_quote(remove_genitive(keep_only_letters(remove_https(s))))\n",
    "    \n",
    "    if pos_tagging:\n",
    "        #cannot use pos tagging without lemmatizing\n",
    "        assert(use_lemmatizing)\n",
    "    \n",
    "    #tokenizing the texts (removing line break, space and capitalization)\n",
    "    token_comm = re.split(\" \", clean_pipeline(txt).strip().lower())\n",
    "    \n",
    "    #to avoid empty token (caused by multiple spaces in the tokenization)\n",
    "    token_comm = [t for t in token_comm if len(t) > 0]\n",
    "    \n",
    "    if pos_tagging:\n",
    "        token_comm = pos_tag(token_comm)\n",
    "    else:\n",
    "        token_comm = zip(token_comm, [None]*len(token_comm))\n",
    "        \n",
    "    #removing all words of three letters or less\n",
    "    bigger_w = [x for x in token_comm if len(x[0]) > 3]\n",
    "\n",
    "    #removing stop_words\n",
    "    wout_sw_w = [x for x in bigger_w if x[0] not in stop_words]\n",
    "    \n",
    "    if pos_tagging and use_lemmatizing:\n",
    "        #get lemma of each word, then return result\n",
    "        return [WordNetLemmatizer().lemmatize(word, get_wordnet_pos(tag)) for word, tag in wout_sw_w]\n",
    "    elif use_lemmatizing:\n",
    "        #get lemma of each word, then return result\n",
    "        return [WordNetLemmatizer().lemmatize(word) for word, _ in wout_sw_w]\n",
    "    else:\n",
    "        return [word for word,_ in wout_sw_w]\n",
    "\n",
    "def perform_lda(documents, n_topics, n_words, alphas, beta, tokens_col):\n",
    "    '''\n",
    "    will perform LDA on a list of documents (== list of token)\n",
    "    assume that documents is a DataFrame with a column of unique id (uid).\n",
    "    \n",
    "    '''\n",
    "    cv = CountVectorizer(inputCol=tokens_col, outputCol=\"raw_features\")\n",
    "    cvmodel = cv.fit(documents)\n",
    "    result_cv = cvmodel.transform(documents)\n",
    "    \n",
    "    #we perform an tf-idf (term frequency inverse document frequency), to avoid threads with a lot of words to pollute the topics.\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(result_cv)\n",
    "    result_tfidf = idfModel.transform(result_cv) \n",
    "    \n",
    "    corpus = result_tfidf.select(\"uid\", \"features\")\n",
    "    \n",
    "    #defining and running the lda. \n",
    "    lda = LDA(k=n_topics, docConcentration=alphas, topicConcentration=beta)\n",
    "    model = lda.fit(corpus)\n",
    "    \n",
    "    #retrieving topics, and the vocabulary constructed by the CountVectorizer\n",
    "    topics = model.describeTopics(maxTermsPerTopic=n_words)\n",
    "    vocab = cvmodel.vocabulary\n",
    "    \n",
    "    #the topics are just numerical indices, we need to convert them to words, and associate them to their weights..\n",
    "    topics_with_weights = topics.rdd.map(lambda r: (r[0], ([(vocab[t],w) for t,w in zip(r[1], r[2])]), ' '.join([vocab[t] for t in r[1]]))).toDF().selectExpr(\"_1 as topic_number\", \"_2 as topic_weight\", \"_3 as topic\")\n",
    "    \n",
    "    return topics_with_weights\n",
    "\n",
    "def display_topics(topics_n_weight):\n",
    "    tops = topics_n_weight.select('topic').collect()\n",
    "    for i in range(len(tops)):\n",
    "        print(\"T %d: %s\"%(i+1, tops[i][0]))\n",
    "    return tops\n",
    "\n",
    "def lda_preprocess(df, use_lemmatizing=False, use_pos_tagging=False):\n",
    "    data_preprocessed = df.select('body', 'created').rdd.filter(lambda r: len(r[0]) > 50).map(lambda r: (text_preprocessing(r[0], en_stop, pos_tagging=use_pos_tagging, use_lemmatizing=use_lemmatizing), r[1])).filter(lambda r: r[0])\n",
    "    data_uid = data_preprocessed.map(lambda r: r).zipWithUniqueId().map(lambda r: (r[0][0], r[1])).toDF().selectExpr('_1 as text', '_2 as uid')\n",
    "    return data_uid\n",
    "    \n",
    "        \n",
    "def lda_display_and_time(df, n_topics, n_words, beta=0.01, alpha=0.01):\n",
    "    print('On %d comments, using %d topics with %d words, beta value : %.3f, alpha value: %.3f'%(df.count(), n_topics, n_words, beta, alpha))\n",
    "    #print('Starting day : %s Ending day : %s'%(df.agg({\"created\": \"min\"}).collect()[0][0], df.agg({\"created\": \"max\"}).collect()[0][0]))\n",
    "    print('Start time : '+ str(datetime.datetime.now()))\n",
    "    alphas = [alpha]*n_topics\n",
    "    topic_n_weights = perform_lda(df, n_topics, n_words, alphas, beta, 'text')\n",
    "    topics_clean = display_topics(topic_n_weights)\n",
    "    print('End time : \\n'+ str(datetime.datetime.now()))\n",
    "    return spark.createDataFrame(topics_clean, schema = StructType([StructField(\"Topic\", StringType())]))\n",
    "\n",
    "\n",
    "def grid_search(df, n_topics, n_words, betas, alphas):\n",
    "    return [(a, b, lda_display_and_time(df, n_topics, n_words, b, a)) for a in alphas for b in betas]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics on week before the election"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out the less upvoted comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2450251"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_trump_score = data_trump.filter(data_trump.score > 10)\n",
    "data_trump_score.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77512"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_week = datetime.date(year=2016, month=10, day=31)\n",
    "end_week = datetime.date(year=2016, month=11, day=8)\n",
    "trump_week_b4_election = data_trump_score.filter(data_trump_score.created > start_week).filter(data_trump_score.created < end_week)\n",
    "trump_week_b4_election.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The week before the election without any subsampling : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50489"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_week_b4_election = lda_preprocess(trump_week_b4_election, use_lemmatizing=True, use_pos_tagging=False)\n",
    "trump_week_b4_election.cache()\n",
    "trump_week_b4_election.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_search(trump_week_b4_election, n_topics=8, n_words=5, betas=[0.01, 0.025, 0.05, 0.075, 0.1], alphas=[0.01, 0.025, 0.05, 0.075, 0.1])\n",
    "trump_lda_res = lda_display_and_time(trump_week_b4_election, n_topics=8, n_words=5, beta=0.1, alpha=0.025)\n",
    "#trump_lda_res.write.mode('overwrite').parquet('../data/trump_lda_result.parquet')\n",
    "trump_lda_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hillary Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hillary_data = spark.read.load('../data/hillary_comments.parquet')\n",
    "hillary_election_week = hillary_data.filter(hillary_data.score > 10).filter(hillary_data.created > start_week).filter(hillary_data.created < end_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hillary_election_prepro = lda_preprocess(hillary_election_week, use_lemmatizing=True, use_pos_tagging=True)\n",
    "print hillary_election_prepro.count()\n",
    "hillary_election_prepro.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_search(hillary_election_prepro, 8, 5, betas=[0.01, 0.025, 0.05, 0.075, 0.1], alphas=[0.01, 0.025])\n",
    "hillary_lda_res = lda_display_and_time(hillary_election_prepro, n_topics=8, n_words=5, beta=0.1, alpha=0.025)\n",
    "hillary_lda_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hillary_lda_res.write.mode('overwrite').parquet('../data/hillary_lda_result.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = spark.read.load('../data/news_week_b4_elec.parquet')\n",
    "news_election_week = news_data.filter(news_data.created > start_week).filter(news_data.created < end_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[text: array<string>, uid: bigint]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_election_prepro = lda_preprocess(news_election_week, use_lemmatizing=True, use_pos_tagging=False)\n",
    "print news_election_prepro.count()\n",
    "news_election_prepro.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3945\n",
      "+--------------------+-----+\n",
      "|                text|  uid|\n",
      "+--------------------+-----+\n",
      "|[always, found, s...|  490|\n",
      "|[anyone, hating, ...| 1867|\n",
      "|[trump, said, ref...| 2596|\n",
      "|[wait, till, hill...| 5431|\n",
      "|[downvoted, stati...| 6241|\n",
      "|[telling, campaig...| 7618|\n",
      "|[vote, hillary, t...| 7861|\n",
      "|[clinton, trying,...| 8509|\n",
      "|[problem, politic...| 8671|\n",
      "|[mention, thing, ...|10696|\n",
      "|[think, republica...|14179|\n",
      "|[public, financin...|17014|\n",
      "|[argument, democr...|23008|\n",
      "|[pas, democrat, e...|23251|\n",
      "|[except, crazy, b...|25438|\n",
      "|[predicted, trump...|27625|\n",
      "|[trump, complain,...|31351|\n",
      "|[thing, shameful,...|31432|\n",
      "|[current, neutere...|32890|\n",
      "|[reason, contain,...|35482|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "election_buzz_words = ['trump', 'donald', 'hillary', 'vote', 'voting', 'election', 'campaign', 'clinton']\n",
    "\n",
    "\n",
    "def election_topic(tokens, buzz_words):\n",
    "    for t in tokens:\n",
    "        if t in buzz_words:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "news_election_focused = news_election_prepro.rdd.filter(lambda r : election_topic(r[0], election_buzz_words)).toDF()\n",
    "print news_election_focused.count()\n",
    "news_election_focused.cache()\n",
    "news_election_focused.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[text: array<string>, uid: bigint]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_election_prepro_sample = news_election_prepro.sample(False, 0.1, 4544)\n",
    "print news_election_prepro_sample.count()\n",
    "news_election_prepro_sample.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 8628 comments, using 8 topics with 5 words, beta value : 0.100, alpha value: 0.025\n",
      "Start time : 2018-12-16 15:18:02.044559\n",
      "T 1: trump page supporter owner shooting\n",
      "T 2: piece million lawyer drought touch\n",
      "T 3: bank cost doctor fire paid\n",
      "T 4: child drug world year thing\n",
      "T 5: school want need really even\n",
      "T 6: woman pipeline court case game\n",
      "T 7: kill muslim shoot dont respect\n",
      "T 8: government information request public tax\n",
      "End time : \n",
      "2018-12-16 15:18:21.482671\n"
     ]
    }
   ],
   "source": [
    "news_lda_res_sample = lda_display_and_time(news_election_prepro_sample, n_topics=8, n_words=5, beta=0.1, alpha=0.025)\n",
    "news_lda_res_sample.write.mode('overwrite').parquet('../data/news_sample_lda_result.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 3945 comments, using 8 topics with 5 words, beta value : 0.100, alpha value: 0.025\n",
      "Start time : 2018-12-16 15:18:21.815268\n",
      "T 1: party want state muslim jew\n",
      "T 2: solar power company utility florida\n",
      "T 3: isi syria saudi syrian dealing\n",
      "T 4: make vote law trump help\n",
      "T 5: trump even black church vote\n",
      "T 6: arizona association sending auto epstein\n",
      "T 7: clinton email trump time know\n",
      "T 8: republican vote government right state\n",
      "End time : \n",
      "2018-12-16 15:18:37.138017\n"
     ]
    }
   ],
   "source": [
    "news_focused_lda_res = lda_display_and_time(news_election_focused, n_topics=8, n_words=5, beta=0.1, alpha=0.025)\n",
    "news_focused_lda_res.write.mode('overwrite').parquet('../data/news_focused_lda_result.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[text: array<string>, uid: bigint]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_election_prepro_score = lda_preprocess(news_election_week.filter(news_election_week.score >10), use_lemmatizing=True, use_pos_tagging=False)\n",
    "print news_election_prepro_score.count()\n",
    "news_election_prepro_score.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 10544 comments, using 8 topics with 5 words, beta value : 0.100, alpha value: 0.025\n",
      "Start time : 2018-12-16 17:51:40.792959\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2548.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 881.0 failed 1 times, most recent failure: Lost task 2.0 in stage 881.0 (TID 30312, localhost, executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2131)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n\tat org.apache.spark.mllib.clustering.OnlineLDAOptimizer.submitMiniBatch(LDAOptimizer.scala:501)\n\tat org.apache.spark.mllib.clustering.OnlineLDAOptimizer.next(LDAOptimizer.scala:450)\n\tat org.apache.spark.mllib.clustering.OnlineLDAOptimizer.next(LDAOptimizer.scala:263)\n\tat org.apache.spark.mllib.clustering.LDA.run(LDA.scala:336)\n\tat org.apache.spark.ml.clustering.LDA.fit(LDA.scala:912)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-4385b6b0ac57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnews_score_lda_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_display_and_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_election_prepro_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnews_score_lda_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/news_score_lda_result.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a75e64e8bc1a>\u001b[0m in \u001b[0;36mlda_display_and_time\u001b[0;34m(df, n_topics, n_words, beta, alpha)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start time : '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0malphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mtopic_n_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperform_lda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0mtopics_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_n_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'End time : \\n'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a75e64e8bc1a>\u001b[0m in \u001b[0;36mperform_lda\u001b[0;34m(documents, n_topics, n_words, alphas, beta, tokens_col)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m#defining and running the lda.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocConcentration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopicConcentration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m#retrieving topics, and the vocabulary constructed by the CountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.3.2-bin-hadoop2.7/python/pyspark/ml/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/Applications/spark-2.3.2-bin-hadoop2.7/python/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.3.2-bin-hadoop2.7/python/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.3.2-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.3.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.3.2-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2548.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 881.0 failed 1 times, most recent failure: Lost task 2.0 in stage 881.0 (TID 30312, localhost, executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2131)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n\tat org.apache.spark.mllib.clustering.OnlineLDAOptimizer.submitMiniBatch(LDAOptimizer.scala:501)\n\tat org.apache.spark.mllib.clustering.OnlineLDAOptimizer.next(LDAOptimizer.scala:450)\n\tat org.apache.spark.mllib.clustering.OnlineLDAOptimizer.next(LDAOptimizer.scala:263)\n\tat org.apache.spark.mllib.clustering.LDA.run(LDA.scala:336)\n\tat org.apache.spark.ml.clustering.LDA.fit(LDA.scala:912)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "news_score_lda_res = lda_display_and_time(news_election_prepro_score, n_topics=8, n_words=5, beta=0.1, alpha=0.025)\n",
    "news_score_lda_res.write.mode('overwrite').parquet('../data/news_score_lda_result.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
