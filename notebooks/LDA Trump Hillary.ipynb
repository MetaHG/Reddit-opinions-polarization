{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources : \n",
    "[General data science article presenting the method](https://medium.com/@connectwithghosh/topic-modelling-with-latent-dirichlet-allocation-lda-in-pyspark-2cb3ebd5678e)\n",
    "\n",
    "[Issue of an user to do LDA on SO](https://stackoverflow.com/questions/42051184/latent-dirichlet-allocation-lda-in-spark#)\n",
    "\n",
    "[Gist of a LDA implementation](https://gist.github.com/Bergvca/a59b127afe46c1c1c479)\n",
    "\n",
    "[SO question about retrieving topic distribution](https://stackoverflow.com/questions/33072449/extract-document-topic-matrix-from-pyspark-lda-model/41515070)\n",
    "\n",
    "[Slides from presentation about recommended parameters for LDA](http://www.phusewiki.org/wiki/images/c/c9/Weizhong_Presentation_CDER_Nov_9th.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "#other words that need to be removed in order to avoid pollution on the topic. (experimental findings on news subreddit)\n",
    "aux_stop_words = ['people', 'would', 'like', 'img', 'jpg', 'imgjpg']\n",
    "\n",
    "en_stop = set(stopwords.words('english')+aux_stop_words)\n",
    "sc = spark.sparkContext\n",
    "sc.broadcast(en_stop)\n",
    "\n",
    "import datetime\n",
    "import re as re\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trump = spark.read.load('../data/donald_comments.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(txt, stop_words, pos_tagging=False, use_lemmatizing=False):\n",
    "    '''\n",
    "    Take care of doing all the text preprocessing for LDA\n",
    "    Only works on english ASCII content. (works on content with accent or such, but filter them out)\n",
    "    '''\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            #this is the default behaviour for lemmatize. \n",
    "            return wordnet.NOUN\n",
    "    \n",
    "    def remove_https(s):\n",
    "        return re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', s, flags=re.UNICODE)\n",
    "    \n",
    "    #keeping only elements relevant to written speech.\n",
    "    def keep_only_letters(s):\n",
    "        return re.sub('[^a-zA-Z \\']+', '', s, flags=re.UNICODE)\n",
    "    \n",
    "    #remove mark of genitif from speech (i.e. \"'s\" associated to nouns)\n",
    "    def remove_genitive(s): \n",
    "        return re.sub('(\\'s)+', '', s, flags=re.UNICODE)\n",
    "    \n",
    "    def clean_pipeline(s): \n",
    "        return remove_genitive(keep_only_letters(remove_https(s)))\n",
    "    \n",
    "    if pos_tagging:\n",
    "        #cannot use pos tagging without lemmatizing\n",
    "        assert(use_lemmatizing)\n",
    "    \n",
    "    #tokenizing the texts (removing line break, space and capitalization)\n",
    "    token_comm = re.split(\" \", clean_pipeline(txt).strip().lower())\n",
    "    \n",
    "    #to avoid empty token (caused by multiple spaces in the tokenization)\n",
    "    token_comm = [t for t in token_comm if len(t) > 0]\n",
    "    \n",
    "    if pos_tagging:\n",
    "        token_comm = pos_tag(token_comm)\n",
    "    else:\n",
    "        token_comm = zip(token_comm, [None]*len(token_comm))\n",
    "        \n",
    "    #removing all words of three letters or less\n",
    "    bigger_w = [x for x in token_comm if len(x[0]) > 3]\n",
    "\n",
    "    #removing stop_words\n",
    "    wout_sw_w = [x for x in bigger_w if x[0] not in stop_words]\n",
    "    \n",
    "    if pos_tagging and use_lemmatizing:\n",
    "        #get lemma of each word, then return result\n",
    "        return [WordNetLemmatizer().lemmatize(word, get_wordnet_pos(tag)) for word, tag in wout_sw_w]\n",
    "    elif use_lemmatizing:\n",
    "        #get lemma of each word, then return result\n",
    "        return [WordNetLemmatizer().lemmatize(word) for word, _ in wout_sw_w]\n",
    "    else:\n",
    "        return [word for word,_ in wout_sw_w]\n",
    "\n",
    "def perform_lda(documents, n_topics, n_words, beta, tokens_col):\n",
    "    '''\n",
    "    will perform LDA on a list of documents (== list of token)\n",
    "    assume that documents is a DataFrame with a column of unique id (uid).\n",
    "    \n",
    "    '''\n",
    "    cv = CountVectorizer(inputCol=tokens_col, outputCol=\"raw_features\")\n",
    "    cvmodel = cv.fit(documents)\n",
    "    result_cv = cvmodel.transform(documents)\n",
    "    \n",
    "    #we perform an tf-idf (term frequency inverse document frequency), to avoid threads with a lot of words to pollute the topics.\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(result_cv)\n",
    "    result_tfidf = idfModel.transform(result_cv) \n",
    "    \n",
    "    corpus = result_tfidf.select(\"uid\", \"features\")\n",
    "    \n",
    "    #defining and running the lda. Em is the best optimizer performance-wise.\n",
    "    #lda = LDA(k=n_topics, optimizer='em', docConcentration=alphas, topicConcentration=beta)\n",
    "    lda = LDA(k=n_topics, topicConcentration=beta)\n",
    "    \n",
    "    model = lda.fit(corpus)\n",
    "    \n",
    "    #retrieving topics, and the vocabulary constructed by the CountVectorizer\n",
    "    topics = model.describeTopics(maxTermsPerTopic=n_words)\n",
    "    vocab = cvmodel.vocabulary\n",
    "    \n",
    "    #the topics are just numerical indices, we need to convert them to words, and associate them to their weights..\n",
    "    topics_with_weights = topics.rdd.map(lambda r: (r[0], ([(vocab[t],w) for t,w in zip(r[1], r[2])]), ' '.join([vocab[t] for t in r[1]]))).toDF().selectExpr(\"_1 as topic_number\", \"_2 as topic_weight\", \"_3 as topic\")\n",
    "    \n",
    "    return topics_with_weights#, topic_distribution\n",
    "\n",
    "def display_topics(topics_n_weight):\n",
    "    tops = topics_n_weight.select('topic').collect()\n",
    "    for i in range(len(tops)):\n",
    "        print(\"T %d: %s\"%(i+1, tops[i][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a subsample without POS-tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19330"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsample_1 = data_trump.sample(False, 0.01, 35).cache()\n",
    "start_date = datetime.date(year=2016, month=10, day=7)\n",
    "end_date = datetime.date(year=2016, month=11, day=8)\n",
    "data_trump_oct_1 = subsample_1.filter(subsample_1.created > start_date).filter(subsample_1.created < end_date).cache()\n",
    "data_trump_oct_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_display_and_time(df, n_topics, n_words, use_lemmatizing=False, use_pos_tagging=False):\n",
    "    print('On %d comments, using %d topics with %d words, pos: %r, lemma: %r'%(df.count(), n_topics, n_words, use_pos_tagging, use_lemmatizing))\n",
    "    print('Start time : '+ str(datetime.datetime.now()))\n",
    "    data_preprocessed = df.select('body', 'created').rdd.filter(lambda r: len(r[0]) > 50).map(lambda r: (text_preprocessing(r[0], en_stop, pos_tagging=use_pos_tagging, use_lemmatizing=use_lemmatizing), r[1])).filter(lambda r: r[0])\n",
    "    \n",
    "    #prevents error from new class args[0] for whatever reason. (Spark is really capricious sometimes)\n",
    "    data_uid = data_preprocessed.map(lambda r: r).zipWithUniqueId().map(lambda r: (r[0][0], r[1])).toDF().selectExpr('_1 as text', '_2 as uid')\n",
    "    topic_n_weights = perform_lda(data_uid, n_topics, n_words, 0.01, 'text')\n",
    "    display_topics(topic_n_weights)\n",
    "    print('End time : '+ str(datetime.datetime.now()))\n",
    "    return topic_n_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 19330 comments, using 5 topics with 8 words, pos: False, lemma: True\n",
      "Start time : 2018-12-14 21:09:57.769352\n",
      "T 1: predator lyin' force murderous clinton hillary crooked plane\n",
      "T 2: cooking propaganda fuck weasel shit fucking mean right\n",
      "T 3: lynch loretta spirit leg maga attorney hillary even\n",
      "T 4: liar vote corrupt trump traitor general woman election\n",
      "T 5: sexual fearlessness fearless coat sick higher foot hillary\n",
      "End time : 2018-12-14 21:29:44.919924\n"
     ]
    }
   ],
   "source": [
    "lda_display_and_time(data_trump_oct_1, 5, 8, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 19330 comments, using 10 topics with 4 words, pos: False, lemma: True\n",
      "Start time : 2018-12-14 21:31:05.961872\n",
      "T 1: crooked murderous lyin' hillary\n",
      "T 2: sexual post year question\n",
      "T 3: lynch loretta fearlessness fearless\n",
      "T 4: liar traitor stay gonna\n",
      "T 5: higher foot county number\n",
      "T 6: rape machine ballot server\n",
      "T 7: force america airplane hillary\n",
      "T 8: leg sick clinton lyin'\n",
      "T 9: trafficker child wallbuild wish\n",
      "T 10: vote corrupt trump predator\n",
      "End time : 2018-12-14 21:50:48.075994\n"
     ]
    }
   ],
   "source": [
    "lda_display_and_time(data_trump_oct_1, 10, 4, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4804"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = datetime.date(year=2016, month=11, day=1)\n",
    "end_date = datetime.date(year=2016, month=11, day=8)\n",
    "data_trump_now_1 = subsample_1.filter(subsample_1.created > start_date).filter(subsample_1.created < end_date).cache()\n",
    "data_trump_now_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 4804 comments, using 5 topics with 6 words, pos: False, lemma: True\n",
      "Start time : 2018-12-14 21:50:48.471876\n",
      "T 1: shit imgjpg need fucking obama wikileaks\n",
      "T 2: traitor weasel maga said even fuck\n",
      "T 3: fearless fearlessness cooking spirit coat great\n",
      "T 4: vote trump child trafficker state take\n",
      "T 5: corrupt hillary come investigation clinton voter\n",
      "End time : 2018-12-14 22:09:37.231152\n"
     ]
    }
   ],
   "source": [
    "lda_display_and_time(data_trump_now_1, 5, 6, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 4804 comments, using 5 topics with 6 words, pos: True, lemma: True\n",
      "Start time : 2018-12-14 22:34:31.459673\n",
      "T 1: child trafficker coat email need maga\n",
      "T 2: fearlessness fearless cook traitor spirit weasel\n",
      "T 3: clinton investigation think happen know year\n",
      "T 4: corrupt vote trump know say support\n",
      "T 5: hillary person might shit world cover\n",
      "End time : 2018-12-14 23:10:04.924694\n"
     ]
    }
   ],
   "source": [
    "lda_display_and_time(data_trump_now_1, 5, 6, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "619922"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "election_eve = datetime.date(year=2016, month=11, day=7)\n",
    "after_election = datetime.date(year=2016, month=11, day=9)\n",
    "data_trump_elec_day = data_trump.filter(data_trump.created < after_election).filter(data_trump.created > start_date)\n",
    "data_trump_elec_day.cache()\n",
    "data_trump_elec_day.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trump_elec_day_sample = data_trump_elec_day.sample(False, 0.17, 42)\n",
    "data_trump_elec_day_sample.cache()\n",
    "election_res = lda_display_and_time(data_trump_elec_day, 7, 5, True, False)\n",
    "election_res.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
