{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Language processing\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Language processing with TextBlob\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = spark.read.load('../sample_parquet/first_1000/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare stopwords, stemmer and lemmatizer for messages preprocessing.\n",
    "en_stopwords = stopwords.words('english')\n",
    "en_stemmer = SnowballStemmer('english')\n",
    "en_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_messages = messages.filter(\"body != '[removed]' and body != '[deleted]'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_body(body, n_grams=1, left_pad_symbol=None, right_pad_symbol=None, lemmatizer=None, stemmer=None, \\\n",
    "                   stop_words=None, lemmatize_stop_words=False, stem_stop_words=False, remove_stop_words=False):\n",
    "    \"\"\"\n",
    "    Process the message bodies of the given rdd\n",
    "        \n",
    "    Parameters:\n",
    "        body: \n",
    "            string message body\n",
    "        n_gram: \n",
    "            size of the n_grams in the rdd output\n",
    "        lemmatizer: \n",
    "            lemmatizer to use on the message words. If None, words are not lemmatize\n",
    "        stemmer: \n",
    "            stemmer to use on the message words. If None, words are not stemmed.\n",
    "        stop_words: \n",
    "            list of words to consider as stop words\n",
    "        lemmatize_stop_words: \n",
    "            boolean to lemmatize stop words\n",
    "        stem_stop_words: \n",
    "            boolean to stem stop words\n",
    "        remove_stop_words: \n",
    "            boolean to remove stop words from the tokens\n",
    "        \n",
    "    Returns:\n",
    "        rdd of the form (parent_id, id, processed_msg_body)\n",
    "    \"\"\"\n",
    "    \n",
    "    if n_grams < 1:\n",
    "        raise ValueError(\"n_grams should be bigger than 1\")\n",
    "    \n",
    "    tknzr = TweetTokenizer()\n",
    "    tokens = tknzr.tokenize(body)\n",
    "    \n",
    "    if stop_words is None:\n",
    "        stop_words = []\n",
    "    if lemmatizer is not None and stemmer is not None:\n",
    "        if remove_stop_words:\n",
    "            tokens = [lemmatizer.lemmatize(stemmer.stem(token)) for token in tokens if token not in stop_words]\n",
    "        elif not lemmatize_stop_words and not stem_stop_words:\n",
    "            tokens = [lemmatizer.lemmatize(stemmer.stem(token)) if token not in stop_words else token for token in tokens]\n",
    "        elif not lemmatize_stop_words:\n",
    "            tokens = [lemmatizer.lemmatize(stemmer.stem(token)) if token not in stop_words else stemmer.stem(token) for token in tokens]\n",
    "        elif not stem_stop_words:\n",
    "            tokens = [lemmatizer.lemmatize(stemmer.stem(token)) if token not in stop_words else lemmatizer.lemmatize(token) for token in tokens]\n",
    "    elif lemmatizer is not None:\n",
    "        if remove_stop_words:\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "        elif not lemmatize_stop_words:\n",
    "            tokens = [lemmatizer.lemmatize(token) if token not in stop_words else token for token in tokens]\n",
    "    elif stemmer is not None:\n",
    "        if remove_stop_words:\n",
    "            tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "        elif not stem_stop_words is not None:\n",
    "            tokens = [stemmer.stem(token) if token not in stop_words else token for token in tokens]\n",
    "    elif stemmer is not None and lemmatizer is not None:\n",
    "            tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    if left_pad_symbol is not None and right_pad_symbol is not None:\n",
    "        tokens = list(nltk.ngrams(tokens, n_grams, True, True, left_pad_symbol, right_pad_symbol))\n",
    "    elif left_pad_symbol is not None:\n",
    "        tokens = list(nltk.ngrams(tokens, n_grams, pad_left=True, left_pad_symbol=left_pad_symbol))\n",
    "    elif right_pad_symbol is not None:\n",
    "        tokens = list(nltk.ngrams(tokens, n_grams, pad_right=True, right_pad_symbol=right_pad_symbol))\n",
    "    else:\n",
    "        tokens = list(nltk.ngrams(tokens, n_grams))\n",
    "\n",
    "    return [list(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.process_body(body, n_grams=1, left_pad_symbol=None, right_pad_symbol=None, lemmatizer=None, stemmer=None, stop_words=None, lemmatize_stop_words=False, stem_stop_words=False, remove_stop_words=False)>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_body_udf = func.udf(process_body, ArrayType(ArrayType(StringType(), False), False))\n",
    "spark.udf.register('process_body', process_body, ArrayType(ArrayType(StringType(), False), False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence polarity using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.compute_nltk_polarity(msg_body)>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_nltk_polarity(msg_body):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    msg_body = sid.polarity_scores(msg_body)\n",
    "    return msg_body\n",
    "\n",
    "compute_nltk_polarity_udf = func.udf(compute_nltk_polarity, MapType(StringType(), FloatType(), False))\n",
    "spark.udf.register('compute_nltk_polarity', compute_nltk_polarity_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_bodies = cleaned_messages.selectExpr('id', 'created_utc', \"compute_nltk_polarity(body) as scores\")\n",
    "sent_nltk_scores = sent_bodies.select('id', 'created_utc', 'scores.neg', 'scores.neu', 'scores.pos')\n",
    "sent_nltk_scores = sent_nltk_scores.toDF('id', 'created_utc', 'nltk_negativity', 'nltk_neutrality', 'nltk_positivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------------+---------------+---------------+\n",
      "|     id|created_utc|nltk_negativity|nltk_neutrality|nltk_positivity|\n",
      "+-------+-----------+---------------+---------------+---------------+\n",
      "|c595rma| 1341368093|          0.065|          0.935|            0.0|\n",
      "|c595rqe| 1341368108|            0.0|            1.0|            0.0|\n",
      "|c595rwc| 1341368128|            0.0|            1.0|            0.0|\n",
      "|c595sdr| 1341368193|            0.0|          0.878|          0.122|\n",
      "|c595sop| 1341368236|            0.0|            1.0|            0.0|\n",
      "|c595sui| 1341368256|            0.0|           0.84|           0.16|\n",
      "|c595t5u| 1341368301|           0.36|          0.443|          0.197|\n",
      "|c595ti7| 1341368347|            0.0|            1.0|            0.0|\n",
      "|c595u4r| 1341368440|            0.0|            1.0|            0.0|\n",
      "|c595ule| 1341368505|          0.377|          0.623|            0.0|\n",
      "|c595up4| 1341368519|            0.0|          0.408|          0.592|\n",
      "|c595v6i| 1341368588|          0.456|          0.544|            0.0|\n",
      "|c595w8b| 1341368743|            0.0|            1.0|            0.0|\n",
      "|c595whq| 1341368782|            0.0|            1.0|            0.0|\n",
      "|c595xbt| 1341368907|          0.121|          0.786|          0.093|\n",
      "|c595yos| 1341369104|          0.147|           0.63|          0.223|\n",
      "|c595ypd| 1341369107|            0.0|          0.385|          0.615|\n",
      "|c595z4z| 1341369174|            0.0|            1.0|            0.0|\n",
      "|c595zjd| 1341369231|          0.111|          0.801|          0.088|\n",
      "|c595zrv| 1341369266|            0.0|          0.408|          0.592|\n",
      "+-------+-----------+---------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_nltk_scores.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence polarity using TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using simple sentence polarity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.compute_blob_polarity(msg_body)>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_blob_polarity(msg_body):\n",
    "    sentiment = TextBlob(msg_body).sentiment\n",
    "    return {'polarity': sentiment.polarity, 'subjectivity': sentiment.subjectivity}\n",
    "\n",
    "compute_blob_polarity_udf = func.udf(compute_blob_polarity, MapType(StringType(), FloatType(), False))\n",
    "spark.udf.register('compute_blob_polarity', compute_blob_polarity_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_blob_bodies = cleaned_messages.selectExpr('id', 'created_utc', \"compute_blob_polarity(body) as scores\")\n",
    "sent_blob_scores = sent_blob_bodies.select('id', 'created_utc', 'scores.polarity', 'scores.subjectivity')\n",
    "sent_blob_scores = sent_blob_scores.toDF('id', 'created_utc', 'text_blob_polarity', 'text_blob_subjectivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+----------------------+\n",
      "|     id|created_utc|text_blob_polarity|text_blob_subjectivity|\n",
      "+-------+-----------+------------------+----------------------+\n",
      "|c595rma| 1341368093|      -0.041666668|                 0.425|\n",
      "|c595rqe| 1341368108|               0.0|                   0.0|\n",
      "|c595rwc| 1341368128|        0.20454545|             0.6818182|\n",
      "|c595sdr| 1341368193|               0.5|                   0.5|\n",
      "|c595sop| 1341368236|               0.0|                   0.0|\n",
      "|c595sui| 1341368256|        0.06666667|                   0.3|\n",
      "|c595t5u| 1341368301|               1.0|                   1.0|\n",
      "|c595ti7| 1341368347|               0.0|                   0.0|\n",
      "|c595u4r| 1341368440|               0.0|                   0.5|\n",
      "|c595ule| 1341368505|       -0.15982144|            0.60892856|\n",
      "|c595up4| 1341368519|        0.43333334|             0.8333333|\n",
      "|c595v6i| 1341368588|       -0.41666666|             0.6666667|\n",
      "|c595w8b| 1341368743|               0.0|                   0.0|\n",
      "|c595whq| 1341368782|          -0.15625|               0.40625|\n",
      "|c595xbt| 1341368907|        -0.6041667|             0.7513889|\n",
      "|c595yos| 1341369104|               0.1|            0.23333333|\n",
      "|c595ypd| 1341369107|               0.4|                   0.5|\n",
      "|c595z4z| 1341369174|               0.0|                   0.0|\n",
      "|c595zjd| 1341369231|           0.03125|            0.46458334|\n",
      "|c595zrv| 1341369266|               0.7|                   0.6|\n",
      "+-------+-----------+------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_blob_scores.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using twitter trained positive/negative naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.compute_blob_class_polarity(msg_body)>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_blob_class_polarity(msg_body):\n",
    "    pol_class = TextBlob(msg_body, analyzer=NaiveBayesAnalyzer()).sentiment\n",
    "    return {'classification': -1 if pol_class.classification == 'neg' else 1, 'p_pos': pol_class.p_pos, 'p_neg': pol_class.p_neg}\n",
    "\n",
    "compute_blob_class_polarity_udf = func.udf(compute_blob_class_polarity, MapType(StringType(), FloatType(), False))\n",
    "spark.udf.register('compute_blob_class_polarity', compute_blob_class_polarity_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_blob_class_bodies = cleaned_messages.selectExpr('id', 'created_utc', \"compute_blob_class_polarity(body) as scores\")\n",
    "sent_blob_class_scores = sent_blob_class_bodies.select('id', 'created_utc', 'scores.classification', 'scores.p_pos', 'scores.p_neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This does not finish, classifier takes too long\n",
    "# sent_blob_class_scores.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other metrics (Vulgarity, hate speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+\n",
      "|     id|created_utc|              tokens|\n",
      "+-------+-----------+--------------------+\n",
      "|c595rma| 1341368093|[[The], [fear], [...|\n",
      "|c595rqe| 1341368108|     [[Upvote], [!]]|\n",
      "|c595rwc| 1341368128|[[It's], [real], ...|\n",
      "|c595sdr| 1341368193|[[What's], [he], ...|\n",
      "|c595sop| 1341368236|[[does], [it], [e...|\n",
      "|c595sui| 1341368256|[[Your], [user], ...|\n",
      "|c595t5u| 1341368301|[[nope], [:D], [b...|\n",
      "|c595ti7| 1341368347|[[Along], [with],...|\n",
      "|c595u4r| 1341368440|[[Those], [both],...|\n",
      "|c595ule| 1341368505|[[If], [you], [ar...|\n",
      "|c595up4| 1341368519|[[Because], [easy...|\n",
      "|c595v6i| 1341368588|[[Does], [no], [o...|\n",
      "|c595w8b| 1341368743|[[Has], [the], [t...|\n",
      "|c595whq| 1341368782|[[Just], [because...|\n",
      "|c595xbt| 1341368907|[[Also], [,], [do...|\n",
      "|c595yos| 1341369104|[[Meh], [,], [it'...|\n",
      "|c595ypd| 1341369107|[[You], [enjoy], ...|\n",
      "|c595z4z| 1341369174|[[[], [Finnish], ...|\n",
      "|c595zjd| 1341369231|[[>], [*], [It's]...|\n",
      "|c595zrv| 1341369266|[[Good], [insight...|\n",
      "+-------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = cleaned_messages.selectExpr('id', 'created_utc', 'process_body(body) as tokens')\n",
    "tokens.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_matches(msg_grams, ref_grams, ref_grams_intensity=None):\n",
    "    msg_grams_joined = [' '.join(msg_gram) for msg_gram in msg_grams]\n",
    "    msg_grams_counter = Counter(msg_grams_joined)\n",
    "    count = 0.0\n",
    "    intensity = 0.0\n",
    "    for i, ref_gram in enumerate(ref_grams):\n",
    "        count = count + msg_grams_counter[ref_gram]\n",
    "        if ref_grams_intensity is not None:\n",
    "            intensity = intensity + msg_grams_counter[ref_gram] * ref_grams_intensity[i]\n",
    "    \n",
    "    if ref_grams_intensity is None:\n",
    "        return count\n",
    "    else: \n",
    "        return {'count':count, 'intensity':intensity}\n",
    "    \n",
    "def df_count_matches(gram_list):\n",
    "    return func.udf(lambda c: count_matches(c, gram_list), FloatType())\n",
    "\n",
    "def df_count_matches_intensity(gram_list, intensity_list):\n",
    "    return func.udf(lambda c: count_matches(c, gram_list, intensity_list), MapType(StringType(), FloatType()))\n",
    "\n",
    "#df_count_matches_udf = func.udf(df_count_matches, FloatType())\n",
    "#df_count_matches_intensity_udf = func.udf(df_count_matches_intensity, MapType(StringType(), FloatType()))\n",
    "\n",
    "#spark.udf.register('df_count_matches', df_count_matches_udf)\n",
    "#spark.udf.register('df_count_matches_intensity', df_count_matches_intensity_udf)\n",
    "\n",
    "def df_count_matches_sql(gram_list, sql_fun_name):\n",
    "    udf = func.udf(lambda c: count_matches(c, gram_list), FloatType())\n",
    "    spark.udf.register(sql_fun_name, udf)\n",
    "\n",
    "def df_count_matches_intensity_sql(gram_list, intensity_list, sql_fun_name):\n",
    "    udf = func.udf(lambda c: count_matches(c, gram_list, intensity_list), MapType(StringType(), FloatType()))\n",
    "    spark.udf.register(sql_fun_name, udf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vulgarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+\n",
      "|      en_bad_words|gram_rank|\n",
      "+------------------+---------+\n",
      "|              2g1c|        1|\n",
      "|     2 girls 1 cup|        4|\n",
      "|    acrotomophilia|        1|\n",
      "|alabama hot pocket|        3|\n",
      "|  alaskan pipeline|        2|\n",
      "|              anal|        1|\n",
      "|         anilingus|        1|\n",
      "|              anus|        1|\n",
      "|           apeshit|        1|\n",
      "|          arsehole|        1|\n",
      "|               ass|        1|\n",
      "|           asshole|        1|\n",
      "|          assmunch|        1|\n",
      "|       auto erotic|        2|\n",
      "|        autoerotic|        1|\n",
      "|          babeland|        1|\n",
      "|       baby batter|        2|\n",
      "|        baby juice|        2|\n",
      "|          ball gag|        2|\n",
      "|        ball gravy|        2|\n",
      "+------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bad_words = spark.read.csv('../bad_words_lexicon/en.csv', header=True)\n",
    "bw_gram_rank = bad_words.withColumn('gram_rank', func.udf(lambda gram: len(gram.split()), IntegerType())(func.col('en_bad_words')))\n",
    "bw_gram_rank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2g1c', 'acrotomophilia', 'anal', 'anilingus', 'anus']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bw_1_grams = [i.en_bad_words for i in bw_gram_rank.filter('gram_rank == 1').select('en_bad_words').collect()]\n",
    "bw_1_grams[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------+\n",
      "|     id|created_utc|nb_bw_matches|\n",
      "+-------+-----------+-------------+\n",
      "|c595rma| 1341368093|          0.0|\n",
      "|c595rqe| 1341368108|          0.0|\n",
      "|c595rwc| 1341368128|          0.0|\n",
      "|c595sdr| 1341368193|          0.0|\n",
      "|c595sop| 1341368236|          0.0|\n",
      "|c595sui| 1341368256|          0.0|\n",
      "|c595t5u| 1341368301|          0.0|\n",
      "|c595ti7| 1341368347|          0.0|\n",
      "|c595u4r| 1341368440|          0.0|\n",
      "|c595ule| 1341368505|          0.0|\n",
      "|c595up4| 1341368519|          0.0|\n",
      "|c595v6i| 1341368588|          0.0|\n",
      "|c595w8b| 1341368743|          0.0|\n",
      "|c595whq| 1341368782|          0.0|\n",
      "|c595xbt| 1341368907|          0.0|\n",
      "|c595yos| 1341369104|          0.0|\n",
      "|c595ypd| 1341369107|          0.0|\n",
      "|c595z4z| 1341369174|          0.0|\n",
      "|c595zjd| 1341369231|          0.0|\n",
      "|c595zrv| 1341369266|          0.0|\n",
      "+-------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bw_counter = tokens.withColumn(\"tokens\", df_count_matches(bw_1_grams)(func.col(\"tokens\"))).withColumnRenamed('tokens', 'nb_bw_matches')\n",
    "bw_counter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hate speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw hate words (basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|  hate_words|gram_rank|\n",
      "+------------+---------+\n",
      "|        gypo|        1|\n",
      "|       gypos|        1|\n",
      "|        cunt|        1|\n",
      "|       cunts|        1|\n",
      "|  peckerwood|        1|\n",
      "| peckerwoods|        1|\n",
      "|     raghead|        1|\n",
      "|    ragheads|        1|\n",
      "|     cripple|        1|\n",
      "|    cripples|        1|\n",
      "|      niggur|        1|\n",
      "|     niggurs|        1|\n",
      "| yellow bone|        2|\n",
      "|yellow bones|        2|\n",
      "|      muzzie|        1|\n",
      "|     muzzies|        1|\n",
      "|      niggar|        1|\n",
      "|     niggars|        1|\n",
      "|      nigger|        1|\n",
      "|     niggers|        1|\n",
      "+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hate_words = spark.read.csv('../hatespeech_lexicon/hatebase_dict.csv', header=True)\n",
    "hate_words = hate_words.withColumnRenamed(\"uncivilised',\", 'hate_words') \\\n",
    "                        .withColumn('hate_words', func.udf(lambda d: d[1:-2])(func.col('hate_words')))\n",
    "hw_gram_rank = hate_words.withColumn('gram_rank', func.udf(lambda gram: len(gram.split()), IntegerType())(func.col('hate_words')))\n",
    "hw_gram_rank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gypo', 'gypos', 'cunt', 'cunts', 'peckerwood']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw_1_grams = [i.hate_words for i in hw_gram_rank.filter('gram_rank == 1').select('hate_words').collect()]\n",
    "hw_1_grams[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------+\n",
      "|     id|created_utc|nb_hw_matches|\n",
      "+-------+-----------+-------------+\n",
      "|c595rma| 1341368093|          0.0|\n",
      "|c595rqe| 1341368108|          0.0|\n",
      "|c595rwc| 1341368128|          0.0|\n",
      "|c595sdr| 1341368193|          0.0|\n",
      "|c595sop| 1341368236|          0.0|\n",
      "|c595sui| 1341368256|          0.0|\n",
      "|c595t5u| 1341368301|          0.0|\n",
      "|c595ti7| 1341368347|          0.0|\n",
      "|c595u4r| 1341368440|          0.0|\n",
      "|c595ule| 1341368505|          1.0|\n",
      "|c595up4| 1341368519|          0.0|\n",
      "|c595v6i| 1341368588|          0.0|\n",
      "|c595w8b| 1341368743|          0.0|\n",
      "|c595whq| 1341368782|          0.0|\n",
      "|c595xbt| 1341368907|          0.0|\n",
      "|c595yos| 1341369104|          0.0|\n",
      "|c595ypd| 1341369107|          0.0|\n",
      "|c595z4z| 1341369174|          0.0|\n",
      "|c595zjd| 1341369231|          0.0|\n",
      "|c595zrv| 1341369266|          0.0|\n",
      "+-------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hw_counter = tokens.withColumn(\"tokens\", df_count_matches(hw_1_grams)(func.col(\"tokens\"))).withColumnRenamed('tokens', 'nb_hw_matches')\n",
    "hw_counter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refined hate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------+\n",
      "|hate_words_ref|intensity|gram_rank|\n",
      "+--------------+---------+---------+\n",
      "|   allah akbar|     0.87|        2|\n",
      "|        blacks|    0.583|        1|\n",
      "|         chink|    0.467|        1|\n",
      "|        chinks|    0.542|        1|\n",
      "|         dykes|    0.602|        1|\n",
      "|        faggot|    0.489|        1|\n",
      "|       faggots|    0.675|        1|\n",
      "|          fags|    0.543|        1|\n",
      "|          homo|    0.667|        1|\n",
      "|        inbred|    0.583|        1|\n",
      "|        nigger|    0.584|        1|\n",
      "|       niggers|    0.672|        1|\n",
      "|        queers|      0.5|        1|\n",
      "|         raped|    0.717|        1|\n",
      "|       savages|    0.778|        1|\n",
      "|         slave|    0.667|        1|\n",
      "|          spic|     0.75|        1|\n",
      "|       wetback|    0.667|        1|\n",
      "|      wetbacks|    0.688|        1|\n",
      "|        whites|    0.556|        1|\n",
      "+--------------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hw_ref_schema = StructType([StructField('hate_words_ref', StringType(), False), StructField('intensity', FloatType(), False)])\n",
    "hate_words_ref = spark.read.csv('../hatespeech_lexicon/refined_ngram_dict.csv', header=True, schema=hw_ref_schema)\n",
    "hw_ref_gram_rank = hate_words_ref.withColumn('gram_rank', func.udf(lambda gram: len(gram.split()), IntegerType())(func.col('hate_words_ref')))\n",
    "hw_ref_gram_rank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['blacks', 'chink', 'chinks', 'dykes', 'faggot'],\n",
       " [0.5830000042915344,\n",
       "  0.46700000762939453,\n",
       "  0.5419999957084656,\n",
       "  0.6019999980926514,\n",
       "  0.48899999260902405])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw_ref_1_grams = [i.hate_words_ref for i in hw_ref_gram_rank.filter('gram_rank == 1').select('hate_words_ref').collect()]\n",
    "hw_ref_1_intensity = [i.intensity for i in hw_ref_gram_rank.filter('gram_rank == 1').select('intensity').collect()]\n",
    "hw_ref_1_grams[0:5], hw_ref_1_intensity[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+-----------------+\n",
      "|     id|created_utc|hate_ref_intensity|nb_hw_ref_matches|\n",
      "+-------+-----------+------------------+-----------------+\n",
      "|c595rma| 1341368093|               0.0|              0.0|\n",
      "|c595rqe| 1341368108|               0.0|              0.0|\n",
      "|c595rwc| 1341368128|               0.0|              0.0|\n",
      "|c595sdr| 1341368193|               0.0|              0.0|\n",
      "|c595sop| 1341368236|               0.0|              0.0|\n",
      "|c595sui| 1341368256|               0.0|              0.0|\n",
      "|c595t5u| 1341368301|               0.0|              0.0|\n",
      "|c595ti7| 1341368347|               0.0|              0.0|\n",
      "|c595u4r| 1341368440|               0.0|              0.0|\n",
      "|c595ule| 1341368505|               0.0|              0.0|\n",
      "|c595up4| 1341368519|               0.0|              0.0|\n",
      "|c595v6i| 1341368588|               0.0|              0.0|\n",
      "|c595w8b| 1341368743|               0.0|              0.0|\n",
      "|c595whq| 1341368782|               0.0|              0.0|\n",
      "|c595xbt| 1341368907|               0.0|              0.0|\n",
      "|c595yos| 1341369104|               0.0|              0.0|\n",
      "|c595ypd| 1341369107|               0.0|              0.0|\n",
      "|c595z4z| 1341369174|               0.0|              0.0|\n",
      "|c595zjd| 1341369231|               0.0|              0.0|\n",
      "|c595zrv| 1341369266|               0.0|              0.0|\n",
      "+-------+-----------+------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hw_ref_counter = tokens.withColumn(\"tokens\", df_count_matches_intensity(hw_ref_1_grams, hw_ref_1_intensity)(func.col(\"tokens\"))).withColumnRenamed('tokens', 'nb_hw_ref_matches')\n",
    "hw_ref_scores = hw_ref_counter.select('id', 'created_utc', 'nb_hw_ref_matches.intensity', 'nb_hw_ref_matches.count')\n",
    "hw_ref_scores = hw_ref_scores.toDF('id', 'created_utc', 'hate_ref_intensity', 'nb_hw_ref_matches')\n",
    "hw_ref_scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_matches_sql(bw_1_grams, 'bw_count_matches')\n",
    "df_count_matches_sql(hw_1_grams, 'hw_count_matches')\n",
    "df_count_matches_intensity_sql(hw_ref_1_grams, hw_ref_1_intensity, 'hw_ref_count_matches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+---------------+---------------+---------------+------------------+----------------------+-------------+-------------+----------------+-----------------+\n",
      "|     id|created_utc|                body|nltk_negativity|nltk_neutrality|nltk_positivity|text_blob_polarity|text_blob_subjectivity|nb_bw_matches|nb_hw_matches|hw_ref_intensity|nb_hw_ref_matches|\n",
      "+-------+-----------+--------------------+---------------+---------------+---------------+------------------+----------------------+-------------+-------------+----------------+-----------------+\n",
      "|c595rma| 1341368093|The fear that it'...|          0.065|          0.935|            0.0|      -0.041666668|                 0.425|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595rqe| 1341368108|             Upvote!|            0.0|            1.0|            0.0|               0.0|                   0.0|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595rwc| 1341368128|It's real, it's i...|            0.0|            1.0|            0.0|        0.20454545|             0.6818182|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595sdr| 1341368193|What's he now... ...|            0.0|          0.878|          0.122|               0.5|                   0.5|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595sop| 1341368236|  does it ever end?\n",
      "|            0.0|            1.0|            0.0|               0.0|                   0.0|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595sui| 1341368256|Your user name ju...|            0.0|           0.84|           0.16|        0.06666667|                   0.3|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595t5u| 1341368301|nope :D  but SC2S...|           0.36|          0.443|          0.197|               1.0|                   1.0|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595ti7| 1341368347|Along with the re...|            0.0|            1.0|            0.0|               0.0|                   0.0|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595u4r| 1341368440|Those both seem s...|            0.0|            1.0|            0.0|               0.0|                   0.5|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595ule| 1341368505|If you are denyin...|          0.377|          0.623|            0.0|       -0.15982144|            0.60892856|          0.0|          1.0|             0.0|              0.0|\n",
      "|c595up4| 1341368519| Because easy karma.|            0.0|          0.408|          0.592|        0.43333334|             0.8333333|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595v6i| 1341368588|Does no one serio...|          0.456|          0.544|            0.0|       -0.41666666|             0.6666667|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595w8b| 1341368743|Has the timing be...|            0.0|            1.0|            0.0|               0.0|                   0.0|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595whq| 1341368782|Just because spec...|            0.0|            1.0|            0.0|          -0.15625|               0.40625|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595xbt| 1341368907|Also, don't feel ...|          0.121|          0.786|          0.093|        -0.6041667|             0.7513889|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595yos| 1341369104|Meh, it's still n...|          0.147|           0.63|          0.223|               0.1|            0.23333333|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595ypd| 1341369107|You enjoy face-si...|            0.0|          0.385|          0.615|               0.4|                   0.5|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595z4z| 1341369174|[Finnish it. Now....|            0.0|            1.0|            0.0|               0.0|                   0.0|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595zjd| 1341369231|&gt;*It's the ind...|          0.111|          0.801|          0.088|           0.03125|            0.46458334|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595zrv| 1341369266|Good insight bro-...|            0.0|          0.408|          0.592|               0.7|                   0.6|          0.0|          0.0|             0.0|              0.0|\n",
      "+-------+-----------+--------------------+---------------+---------------+---------------+------------------+----------------------+-------------+-------------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp_metrics_df = cleaned_messages.selectExpr('id', 'created_utc', 'body', 'process_body(body) as tokens')\n",
    "nlp_metrics_df = nlp_metrics_df.selectExpr('id', 'created_utc', 'body', \"compute_nltk_polarity(body) as nltk_scores\", \"compute_blob_polarity(body) as blob_scores\", \"bw_count_matches(tokens) as nb_bw_matches\", \"hw_count_matches(tokens) as nb_hw_matches\", \"hw_ref_count_matches(tokens) as hw_ref_matches\")\n",
    "nlp_metrics_df = nlp_metrics_df.selectExpr('id', 'created_utc', 'body', 'nltk_scores.neg as nltk_negativity', 'nltk_scores.neu as nltk_neutrality', 'nltk_scores.pos as nltk_positivity', 'blob_scores.polarity as text_blob_polarity', 'blob_scores.subjectivity as text_blob_subjectivity', 'nb_bw_matches', 'nb_hw_matches', 'hw_ref_matches.intensity as hw_ref_intensity', 'hw_ref_matches.count as nb_hw_ref_matches')\n",
    "nlp_metrics_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_metrics_df = nlp_metrics_df.withColumn('created_utc', func.from_unixtime(nlp_metrics_df['created_utc'], 'yyyy-MM-dd HH:mm:ss.SS').cast(DateType())) \\\n",
    "                               .withColumnRenamed('created_utc', 'creation_date')\n",
    "\n",
    "nlp_metrics_df.registerTempTable(\"nlp_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pandas = nlp_metrics_df.drop('body').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_nlp_metrics = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    creation_date,\n",
    "    \n",
    "    AVG(sum_nltk_negativity) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS nltk_negativity_60d_avg,\n",
    "    \n",
    "    AVG(sum_nltk_neutrality) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS nltk_neutrality_60d_avg,\n",
    "    \n",
    "    AVG(sum_nltk_positivity) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS nltk_positivity_60d_avg,\n",
    "    \n",
    "    AVG(sum_text_blob_polarity) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS text_blob_polarity_60d_avg,\n",
    "    \n",
    "    AVG(sum_text_blob_subjectivity) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS text_blob_subjectivity_60d_avg,\n",
    "    \n",
    "    AVG(sum_nb_bw_matches) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS nb_bw_matches_60d_avg,\n",
    "    \n",
    "    AVG(sum_nb_hw_matches) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS nb_hw_matches_60d_avg,\n",
    "    \n",
    "    AVG(sum_hw_ref_intensity) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS hw_ref_intensity_60d_avg,\n",
    "    \n",
    "    AVG(sum_nb_hw_ref_matches) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS nb_hw_ref_matches_60d_avg\n",
    "    \n",
    "FROM (\n",
    "    SELECT\n",
    "        creation_date,\n",
    "        SUM(nltk_negativity) AS sum_nltk_negativity,\n",
    "        SUM(nltk_neutrality) AS sum_nltk_neutrality,\n",
    "        SUM(nltk_positivity) AS sum_nltk_positivity,\n",
    "        SUM(text_blob_polarity) AS sum_text_blob_polarity,\n",
    "        SUM(text_blob_subjectivity) AS sum_text_blob_subjectivity, \n",
    "        SUM(nb_bw_matches) AS sum_nb_bw_matches,\n",
    "        SUM(nb_hw_matches) AS sum_nb_hw_matches,\n",
    "        SUM(hw_ref_intensity) AS sum_hw_ref_intensity,\n",
    "        SUM(nb_hw_ref_matches) AS sum_nb_hw_ref_matches\n",
    "    FROM nlp_metrics\n",
    "    GROUP BY creation_date\n",
    "    ORDER BY creation_date\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset starts on the 2005-12-12 and ends on the 2017-04-01.\n"
     ]
    }
   ],
   "source": [
    "date_extrema = spark.sql(\"\"\"\n",
    "SELECT MIN(creation_date), MAX(creation_date)\n",
    "FROM nlp_metrics\n",
    "\"\"\").collect()\n",
    "min_date = date_extrema[0][0]\n",
    "max_date = date_extrema[0][1]\n",
    "\n",
    "print(\"The dataset starts on the {} and ends on the {}.\".format(min_date, max_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 5757823 messages\n"
     ]
    }
   ],
   "source": [
    "total_messages = spark.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM nlp_metrics\n",
    "\"\"\").collect()\n",
    "\n",
    "tot_msg = total_messages[0][0]\n",
    "\n",
    "print(\"The dataset contains {} messages\".format(tot_msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over all the messages, there are at least 589284.0 bad words, 106483.0 hate speech words, 15617.0 refined hate speech words\n"
     ]
    }
   ],
   "source": [
    "total_no_respect = spark.sql(\"\"\"\n",
    "SELECT SUM(nb_bw_matches), SUM(nb_hw_matches), SUM(nb_hw_ref_matches)\n",
    "FROM nlp_metrics\n",
    "\"\"\").collect()\n",
    "\n",
    "sum_bw = total_no_respect[0][0]\n",
    "sum_hw = total_no_respect[0][1]\n",
    "sum_ref_hw = total_no_respect[0][2]\n",
    "\n",
    "print(\"Over all the messages, there are at least {} bad words, {} hate speech words, {} refined hate speech words\".format(sum_bw, sum_hw, sum_ref_hw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_polarity = spark.sql(\"\"\"\n",
    "SELECT SUM(nltk_negativity), SUM(nltk_neutrality), SUM(nltk_positivity), SUM(text_blob_polarity), SUM(text_blob_subjectivity)\n",
    "FROM nlp_metrics\n",
    "\"\"\").collect()\n",
    "\n",
    "sum_nltk_neg = total_polarity[0][0]\n",
    "sum_nltk_neu = total_polarity[0][1]\n",
    "sum_nltk_pos = total_polarity[0][2]\n",
    "sum_txt_blob_pol = total_polarity[0][3]\n",
    "sum_txt_blob_subj = total_polarity[0][4]\n",
    "\n",
    "print(\"The total pos/neu/neg score of the dataset is: negativity: {}, neutrality:{}, positivity: {}\".format(nltk_neg, nltk_neu, nltk_pos))\n",
    "print(\"The total polarity and subjectivity score of the dataset is: polarity: {}, subjectivity: {}\".format(sum_txt_blob_pol, sum_txt_blob_subj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
