{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Language processing\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Language processing with TextBlob\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from plot_correlation import plot_corr_mats, plot_metrics_distrib, plot_nlp_daily_metrics, plot_nlp_daily_distrib, plot_nlp_sample_distrib\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_to_pandas(spark_metrics):\n",
    "    metrics_pd = spark_metrics.toPandas()\n",
    "    metrics_pd = metrics_pd.set_index('creation_date')\n",
    "    metrics_pd = metrics_pd.sort_index()\n",
    "    return metrics_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(filename, sql_query, sql_table_name):\n",
    "    spark_metrics = spark.read.load(filename)\n",
    "    spark_metrics.registerTempTable(sql_table_name)\n",
    "    avg_metrics = spark.sql(sql_query)\n",
    "\n",
    "    pd_metrics = spark_to_pandas(spark_metrics)\n",
    "    pd_metrics_avg = spark_to_pandas(avg_metrics)\n",
    "    pd_metrics_n = pd_metrics.div(pd_metrics['msg_count'], axis=0)\n",
    "    pd_metrics_avg_n = pd_metrics_avg.div(pd_metrics_avg['msg_count_60d_avg'], axis=0)\n",
    "    \n",
    "    return pd_metrics_n, pd_metrics_avg_n, pd_metrics, pd_metrics_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_full_name = '../data/nlp_nltk_metrics_daily_full_0.01.parquet/'\n",
    "nltk_15_17_name = '../data/nlp_nltk_metrics_daily_15_17_0.1.parquet/'\n",
    "\n",
    "nltk_sql_table_name = 'nltk_metrics'\n",
    "nltk_sql_query = (f\"\"\"\n",
    "SELECT\n",
    "    creation_date,\n",
    "    \n",
    "    AVG(msg_count) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS msg_count_60d_avg,\n",
    "\n",
    "    AVG(sum_nltk_negativity) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS nltk_negativity_60d_avg,\n",
    "\n",
    "    AVG(sum_nltk_neutrality) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS nltk_neutrality_60d_avg,\n",
    "    \n",
    "    AVG(sum_nltk_positivity) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS nltk_positivity_60d_avg\n",
    "\n",
    "FROM {nltk_sql_table_name}\n",
    "\"\"\")\n",
    "\n",
    "def plot_nltk_metrics(filename, sql_query, sql_table_name):\n",
    "    m_n, m_avg_n, m, m_avg = get_metrics(filename, sql_query, sql_table_name)\n",
    "    \n",
    "    def plot_averages(neg=True, neu=True, pos=True):\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        title = None\n",
    "        labels = []\n",
    "        if neg:\n",
    "            title = 'Negativity avg'\n",
    "            labels.append('Negativity avg')\n",
    "            plt.plot(m_avg_n['nltk_negativity_60d_avg'])\n",
    "        if neu:\n",
    "            title = 'Neutrality avg' if title is None else title + ' VS Neutrality avg'\n",
    "            labels.append('Neutrality avg')\n",
    "            plt.plot(m_avg_n['nltk_neutrality_60d_avg'])\n",
    "        if pos:\n",
    "            title = 'Positivity avg' if title is None else title + ' VS Positivity avg'\n",
    "            labels.append('Positivity avg')\n",
    "            plt.plot(m_avg_n['nltk_positivity_60d_avg'])\n",
    "        plt.title(title + ' scores over time')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend(labels)\n",
    "    \n",
    "    def plot_stat(neg=False, neu=False, pos=False):\n",
    "        if neg or neu or pos:\n",
    "            plt.figure(figsize=(20, 10))\n",
    "        if neg:\n",
    "            plt.plot(m_n['sum_nltk_negativity'])\n",
    "            plt.plot(m_avg_n['nltk_negativity_60d_avg'])\n",
    "            plt.title('Negativity scores over time')\n",
    "            plt.legend(['Negativity', 'Negativity avg'])\n",
    "        elif neu:\n",
    "            plt.plot(m_n['sum_nltk_neutrality'])\n",
    "            plt.plot(m_avg_n['nltk_neutrality_60d_avg'])\n",
    "            plt.title('Neutrality scores over time')\n",
    "            plt.legend(['Neutrality', 'Neutrality avg'])\n",
    "        elif pos:\n",
    "            plt.plot(m_n['sum_nltk_positivity'])\n",
    "            plt.plot(m_avg_n['nltk_positivity_60d_avg'])\n",
    "            plt.title('Positivity scores over time')\n",
    "            plt.legend(['Positivity', 'Positivity avg'])\n",
    "        else:\n",
    "            print('Please set one of the following parameter to True: [neg, neu, pos]')\n",
    "            return\n",
    "        \n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Score')\n",
    "            \n",
    "    return plot_averages, plot_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK 2005 - 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_plot_averages, nltk_plot_stat = plot_nltk_metrics(nltk_full_name, nltk_sql_query, nltk_sql_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nltk_plot_averages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_plot_averages(neu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nltk_plot_averages(neg=False, pos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nltk_plot_stat(neg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_plot_stat(neu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nltk_plot_stat(pos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK 2015 - 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_plot_averages, nltk_plot_stat = plot_nltk_metrics(nltk_15_17_name, nltk_sql_query, nltk_sql_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_plot_averages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_plot_averages(neu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_plot_averages(neg=False, pos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_plot_stat(neg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_plot_stat(neu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_plot_stat(pos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_full_name = '../data/nlp_blob_metrics_daily_full_0.01.parquet/'\n",
    "blob_15_17_name = '../data/nlp_blob_metrics_daily_15_17_0.1.parquet/'\n",
    "\n",
    "blob_sql_table_name = 'blob_metrics'\n",
    "blob_sql_query = (f\"\"\"\n",
    "SELECT\n",
    "    creation_date,\n",
    "    \n",
    "    AVG(msg_count) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS msg_count_60d_avg,\n",
    "\n",
    "    AVG(sum_blob_polarity) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS text_blob_polarity_60d_avg,\n",
    "\n",
    "    AVG(sum_blob_subjectivity) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS text_blob_subjectivity_60d_avg\n",
    "\n",
    "FROM {blob_sql_table_name}\n",
    "\"\"\")\n",
    "\n",
    "def plot_blob_metrics(filename, sql_query, sql_table_name):\n",
    "    m_n, m_avg_n, m, m_avg = get_metrics(filename, sql_query, sql_table_name)\n",
    "    \n",
    "    def plot_stat(pol=False, pol_avg=False, subj=False, subj_avg=False):\n",
    "        if pol_avg or pol or subj_avg or subj:\n",
    "            plt.figure(figsize=(20, 10))\n",
    "            title = None\n",
    "            labels = []\n",
    "            if pol:\n",
    "                plt.plot(m_n['sum_blob_polarity'])\n",
    "                title = 'Polarity'\n",
    "                labels.append('Polarity')\n",
    "            if pol_avg:\n",
    "                plt.plot(m_avg_n['text_blob_polarity_60d_avg'])\n",
    "                title = 'Polarity avg' if title is None else title + ' VS Polarity avg'\n",
    "                labels.append('Polarity avg')\n",
    "            if subj:\n",
    "                plt.plot(m_n['sum_blob_subjectivity'])\n",
    "                title = 'Subjectivity' if title is None else title + ' VS Subjectivity'\n",
    "                labels.append('Subjectivity')\n",
    "            if subj_avg:\n",
    "                plt.plot(m_avg_n['text_blob_subjectivity_60d_avg'])\n",
    "                title = 'Subjectivity avg' if title is None else title + ' VS Subjectivity avg'\n",
    "                labels.append('Subjectivity avg')\n",
    "            plt.title(title + ' scores over time')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Score')\n",
    "            plt.legend(labels)\n",
    "        else:\n",
    "            print('Please set one of the following parameter to True: [pol, pol_avg, subj, subj_avg]')\n",
    "            \n",
    "    return plot_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blob 2005 - 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_plot_stat = plot_blob_metrics(blob_full_name, blob_sql_query, blob_sql_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blob_plot_stat(pol=True, pol_avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_plot_stat(pol_avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_plot_stat(subj=True, subj_avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_plot_stat(subj_avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blob 2015-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_plot_stat = plot_blob_metrics(blob_15_17_name, blob_sql_query, blob_sql_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_plot_stat(pol=True, pol_avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blob_plot_stat(subj=True, subj_avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_plot_stat(pol_avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_plot_stat(subj_avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad words metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_full_name = '../data/nlp_bw_metrics_daily_full_0.01.parquet/'\n",
    "bw_15_17_name = '../data/nlp_bw_metrics_daily_15_17_0.1.parquet/'\n",
    "\n",
    "bw_sql_table_name = 'bw_metrics'\n",
    "bw_sql_query = (f\"\"\"\n",
    "SELECT\n",
    "    creation_date,\n",
    "    \n",
    "    AVG(msg_count) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS msg_count_60d_avg,\n",
    "\n",
    "    AVG(sum_nb_bw_matches) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS nb_bw_matches_60d_avg\n",
    "\n",
    "FROM {bw_sql_table_name}\n",
    "\"\"\")\n",
    "\n",
    "def plot_bw_metrics(filename, sql_query, sql_table_name):\n",
    "    m_n, m_avg_n, m, m_avg = get_metrics(filename, sql_query, sql_table_name)\n",
    "    \n",
    "    def plot_stat(bw=False, bw_avg=False):\n",
    "        if bw or bw_avg:\n",
    "            plt.figure(figsize=(20, 10))\n",
    "            title = None\n",
    "            labels = []\n",
    "            if bw:\n",
    "                plt.plot(m_n['sum_nb_bw_matches'])\n",
    "                title = 'Bad words'\n",
    "                labels.append('Bad words')\n",
    "            if bw_avg:\n",
    "                plt.plot(m_avg_n['nb_bw_matches_60d_avg'])\n",
    "                title = 'Bad words avg' if title is None else title + ' VS Bad words avg'\n",
    "                labels.append('Bad words avg')\n",
    "                \n",
    "            plt.title(title + ' scores over time')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Score')\n",
    "            plt.legend(labels)\n",
    "        else:\n",
    "            print('Please set one of the following parameter to True: [bw, bw_avg]')\n",
    "            \n",
    "    return plot_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bad words 2005-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_plot_stat = plot_bw_metrics(bw_full_name, bw_sql_query, bw_sql_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bw_plot_stat(bw=True, bw_avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_plot_stat(bw_avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bad words 2015-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_plot_stat = plot_bw_metrics(bw_15_17_name, bw_sql_query, bw_sql_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_plot_stat(bw=True, bw_avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_plot_stat(bw_avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hate words metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_full_name = '../data/nlp_hw_metrics_daily_full_0.01.parquet/'\n",
    "hw_15_17_name = '../data/nlp_hw_metrics_daily_15_17_0.1.parquet/'\n",
    "\n",
    "hw_sql_table_name = 'hw_metrics'\n",
    "hw_sql_query = (f\"\"\"\n",
    "SELECT\n",
    "    creation_date,\n",
    "    \n",
    "    AVG(msg_count) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS msg_count_60d_avg,\n",
    "\n",
    "    AVG(sum_nb_hw_matches) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS nb_hw_matches_60d_avg\n",
    "\n",
    "FROM {hw_sql_table_name}\n",
    "\"\"\")\n",
    "\n",
    "def plot_hw_metrics(filename, sql_query, sql_table_name):\n",
    "    m_n, m_avg_n, m, m_avg = get_metrics(filename, sql_query, sql_table_name)\n",
    "    \n",
    "    def plot_stat(hw=False, hw_avg=False):\n",
    "        if hw or hw_avg:\n",
    "            plt.figure(figsize=(20, 10))\n",
    "            title = None\n",
    "            labels = []\n",
    "            if hw:\n",
    "                plt.plot(m_n['sum_nb_hw_matches'])\n",
    "                title = 'Hate words'\n",
    "                labels.append('Hate words')\n",
    "            if hw_avg:\n",
    "                plt.plot(m_avg_n['nb_hw_matches_60d_avg'])\n",
    "                title = 'Hate words avg' if title is None else title + ' VS Hate words avg'\n",
    "                labels.append('Hate words avg')\n",
    "            plt.title(title + ' scores over time')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Score')\n",
    "            plt.legend(labels)\n",
    "        else:\n",
    "            print('Please set one of the following parameter to True: [hw, hw_avg]')\n",
    "            \n",
    "    return plot_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hate words 2005-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_plot_stat = plot_hw_metrics(hw_full_name, hw_sql_query, hw_sql_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_plot_stat(hw=True, hw_avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hw_plot_stat(hw_avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hate words 2015-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_plot_stat = plot_hw_metrics(hw_15_17_name, hw_sql_query, hw_sql_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_plot_stat(hw=True, hw_avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_plot_stat(hw_avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refined Hate Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_hw_full_name = '../data/nlp_ref_hw_metrics_daily_full_0.01.parquet/'\n",
    "ref_hw_15_17_name = '../data/nlp_ref_hw_metrics_daily_15_17_0.1.parquet/'\n",
    "\n",
    "ref_hw_sql_table_name = 'ref_hw_metrics'\n",
    "ref_hw_sql_query = (f\"\"\"\n",
    "SELECT\n",
    "    creation_date,\n",
    "    \n",
    "    AVG(msg_count) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS msg_count_60d_avg,\n",
    "    \n",
    "    AVG(sum_hw_ref_intensity) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS hw_ref_intensity_60d_avg,\n",
    "\n",
    "    AVG(sum_nb_hw_ref_matches) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS nb_hw_ref_matches_60d_avg\n",
    "\n",
    "FROM {ref_hw_sql_table_name}\n",
    "\"\"\")\n",
    "\n",
    "def plot_ref_hw_metrics(filename, sql_query, sql_table_name):\n",
    "    m_n, m_avg_n, m, m_avg = get_metrics(filename, sql_query, sql_table_name)\n",
    "    \n",
    "    def plot_stat(ref_hw=False, ref_hw_avg=False, intensity=False, intensity_avg=False):\n",
    "        if ref_hw or ref_hw_avg or intensity or intensity_avg:\n",
    "            plt.figure(figsize=(20, 10))\n",
    "            title = None\n",
    "            labels = []\n",
    "            if ref_hw:\n",
    "                plt.plot(m_n['sum_nb_hw_ref_matches'])\n",
    "                title = 'Hate words refined'\n",
    "                labels.append('Hate words refined')\n",
    "            if ref_hw_avg:\n",
    "                plt.plot(m_avg_n['nb_hw_ref_matches_60d_avg'])\n",
    "                title = 'Hate words refined avg' if title is None else title + ' VS Hate words refined avg'\n",
    "                labels.append('Hate words refined avg')\n",
    "            if intensity:\n",
    "                plt.plot(m_n['sum_hw_ref_intensity'])\n",
    "                title = 'Hate words refined intensity' if title is None else title + ' VS Hate words refined intensity'\n",
    "                labels.append('Hate words refined intensity')\n",
    "            if intensity_avg:\n",
    "                plt.plot(m_avg_n['hw_ref_intensity_60d_avg'])\n",
    "                title = 'Hate words refined intensity avg' if title is None else title + ' VS Hate words refined intensity avg'\n",
    "                labels.append('Hate words refined intensity avg')\n",
    "            \n",
    "            plt.title(title + ' scores over time')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Score')\n",
    "            plt.legend(labels)\n",
    "        else:\n",
    "            print('Please set one of the following parameter to True: [ref_hw, ref_hw_avg, intensity, intensity_avg]')\n",
    "            \n",
    "    return plot_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_hw_plot_stat = plot_ref_hw_metrics(ref_hw_full_name, ref_hw_sql_query, ref_hw_sql_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_hw_plot_stat(ref_hw_avg=True, intensity_avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_hw_plot_stat(ref_hw=True, ref_hw_avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_hw_plot_stat(ref_hw_avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_hw_plot_stat(intensity=True, intensity_avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ref_hw_plot_stat(intensity_avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_n, nltk_avg_n, _, _ = get_metrics(nltk_full_name, nltk_sql_query, nltk_sql_table_name)\n",
    "blob_n, blob_avg_n, _, _ = get_metrics(blob_full_name, blob_sql_query, blob_sql_table_name)\n",
    "bw_n, bw_avg_n, _, _ = get_metrics(bw_full_name, bw_sql_query, bw_sql_table_name)\n",
    "hw_n, hw_avg_n, _, _ = get_metrics(hw_full_name, hw_sql_query, hw_sql_table_name)\n",
    "hw_ref_n, hw_ref_avg_n, _, _ = get_metrics(ref_hw_full_name, ref_hw_sql_query, ref_hw_sql_table_name)\n",
    "nlp_n = pd.concat([nltk_n, blob_n, bw_n, hw_n, hw_ref_n], axis=1, sort=True).drop('msg_count', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_n.columns = ['neg', 'neu', 'pos', 'pol', 'subj', 'bw', 'hw', 'hw_ref', 'intensity']\n",
    "pd.to_pickle(nlp_n, '../data/daily_nlp_metrics_merged.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_n = pd.read_pickle('../data/daily_nlp_metrics_merged.pkl').dropna()\n",
    "# Remove outliers assuming gaussian distribution\n",
    "nlp_n_cleaned = nlp_n[(sp.stats.zscore(nlp_n) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_nlp_daily_distrib([nlp_n, nlp_n_cleaned], ['NLP', 'NLP cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_n.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_mats([nlp_n, nlp_n_cleaned], 'NLP metrics', ['pearson', 'spearman'], ['', ' cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_nlp_daily_metrics([nlp_n, nlp_n_cleaned], ['NLP', 'NLP cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample full messages plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_full = spark.read.load('../data/subreddit_nlp_full_0.001.parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sample = nlp_full.sample(withReplacement=False, fraction=0.4, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_nlp_sample = nlp_sample.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_sample = pd_nlp_sample.drop(['id', 'subreddit', 'subreddit_id', 'creation_date', 'body', 'hw_ref_intensity'], axis=1)\n",
    "pd_sample.columns = ['neg', 'neu', 'pos', 'pol', 'subj', 'bw', 'hw', 'hw_ref']\n",
    "pd_sample.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_nlp_sample_distrib([pd_sample], ['NLP sample'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
