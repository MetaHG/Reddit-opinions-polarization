{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "comments = spark.read.load('../data/sample.parquet')\n",
    "comments = comments.withColumn('created', func.from_unixtime(comments['created_utc'], 'yyyy-MM-dd HH:mm:ss.SS').cast(DateType()))\n",
    "comments.registerTempTable(\"comments\")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fetching post content from reddit using praw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kullan-at posta hesabı\n",
      "\n",
      "\n",
      "SAP ABAP Tutorials and Examples\n",
      "\n",
      "\n",
      "Why the story with 165 diggs will never see the light of day (editor control at Digg)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "from praw.models import Submission\n",
    "\n",
    "def retrieve_post_content(reddit_instance, post_id):\n",
    "    if post_id[2] == '_':\n",
    "        #the type identifier of the content has not been removed\n",
    "        post_id = post_id[3:]\n",
    "        \n",
    "    post = Submission(reddit_instance, post_id)\n",
    "    #the title is concatenated to the potential post content.\n",
    "    return post.title +'\\n\\n'+ post.selftext\n",
    "\n",
    "test_ids = ['t3_25xw','t3_cszs','t3_glz0']\n",
    "reddit = praw.Reddit(client_id='PBYOhEGg-oACHw',\n",
    "                     client_secret='zY2HF49pG_LAgauDlCnfmckUwz0',\n",
    "                     user_agent='Fetch post content bot 1.0 by /u/raindust')\n",
    "\n",
    "for t in test_ids:\n",
    "    print(retrieve_post_content(reddit, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA with 1 topic 2 words on sample of comments on the month preceding the USA 2016 election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import gensim\n",
    "from spacy.lang.en import English\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import datetime\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "parser = English()\n",
    "\n",
    "start_date = datetime.date(year=2016, month=10, day=7)\n",
    "end_date = datetime.date(year=2016, month=11, day=7)\n",
    "\n",
    "oct_2016_news_comments = comments.select('link_id','body','created', 'subreddit').filter(comments.created > start_date).filter(comments.created < end_date).filter(comments.subreddit == 'news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_on_posts(dataset, words_per_topic, n_topics, parser=English(), stop_words=en_stop):\n",
    "    '''   \n",
    "    This function performs a LDA (Latent Dirichlet Allocation) model on a set of reddit comments.\n",
    "    Useful for topic modelling/extraction from a reddit post.\n",
    "    Parameters\n",
    "    −−−−−−−−−−\n",
    "    dataset: pyspark RDD or Dataframe, schema should have only two data type : \n",
    "              the post id (link_id) and the body of the comment in this order.\n",
    "              \n",
    "    words_per_topic: number of words that should constitute a topic per post.\n",
    "    \n",
    "    n_topics: number of topics to extract by post\n",
    "    \n",
    "    parser: the natural language parser used, corresponds to a language normally,\n",
    "            by default english (as it is the most used language on reddit).\n",
    "            should be a parser from the spacy.lang library.\n",
    "    \n",
    "    stop_words: set of words that constitutes stop words (i.e. that should be\n",
    "                removed from the tokens)\n",
    "\n",
    "    Returns\n",
    "    −−−−−−−\n",
    "    A RDD with the following pair of data as rows : (<post_id>, <topic (as a list of words)>)) \n",
    "    '''\n",
    "    #useful functions for preprocessing the data for LDA\n",
    "    def tokenize(text):\n",
    "        lda_tokens = []\n",
    "        tokens = parser(text)\n",
    "        for token in tokens:\n",
    "            if token.orth_.isspace():\n",
    "                continue\n",
    "            elif token.like_url:\n",
    "                lda_tokens.append('URL')\n",
    "            elif token.orth_.startswith('@'):\n",
    "                lda_tokens.append('SCREEN_NAME')\n",
    "            else:\n",
    "                lda_tokens.append(token.lower_)\n",
    "        return lda_tokens\n",
    "\n",
    "    def get_lemma(word):\n",
    "        lemma = wn.morphy(word)\n",
    "        if lemma is None:\n",
    "            return word\n",
    "        else:\n",
    "            return lemma\n",
    "\n",
    "    def get_lemma2(word):\n",
    "        return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "    def prepare_text_for_lda(text):\n",
    "        tokens = tokenize(text)\n",
    "        tokens = [token for token in tokens if len(token) > 4]\n",
    "        tokens = [token for token in tokens if token not in en_stop]\n",
    "        tokens = [get_lemma(token) for token in tokens]\n",
    "        return tokens\n",
    "    \n",
    "    def get_n_topics(text_data):\n",
    "        dictionary = gensim.corpora.Dictionary(text_data)\n",
    "        corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = n_topics, id2word=dictionary, passes=15)\n",
    "        topics = ldamodel.print_topics(num_words=words_per_topic)\n",
    "        return topics\n",
    "    \n",
    "    def extract_key_words(lda_result):\n",
    "        return re.findall(r'\\\"(.*?)\\\"', lda_result)\n",
    "\n",
    "    #detecting type of the input given.\n",
    "    if isinstance(dataset, pyspark.sql.dataframe.DataFrame):\n",
    "        dataset = dataset.rdd\n",
    "    elif not isinstance(dataset, pyspark.rdd.RDD):\n",
    "        raise ValueError('Wrong type of dataset, must be either a pyspark RDD or pyspark DataFrame')\n",
    "    \n",
    "    #TODO : keep the minimum timestamp (r[2] of the dataset) during computations.\n",
    "    \n",
    "    #filtering comments that were removed, to avoid them to pollute the topics extracted\n",
    "    filter_absent_comm = dataset.filter(lambda r: r[1] != '[removed]' and r[1] != '[deleted]')\n",
    "    \n",
    "    #applying text preprocesisng for LDA + filtering all empty sets (without tokens as the result of the LDA preprocessing)\n",
    "    LDA_preprocessed = filter_absent_comm.map(lambda r: (r[0], list(prepare_text_for_lda(r[1])))).filter(lambda r: r[1])\n",
    "    \n",
    "    #groupy every comments by post/thread id.\n",
    "    post_and_list_token = LDA_preprocessed.groupByKey().map(lambda x : (x[0], list(x[1])))\n",
    "    \n",
    "    #generating n topics per post/thread.\n",
    "    res_lda = post_and_list_token.map(lambda r: (r[0],get_n_topics(r[1]))).flatMap(lambda r: [(r[0], t) for t in r[1]])\n",
    "    \n",
    "    return res_lda.map(lambda r: (r[0], extract_key_words(r[1][1])))\n",
    "\n",
    "#res = lda_on_posts(comments_about_usa_election.rdd.map(lambda r: (r[2], r[0])), 2, 1)\n",
    "res = lda_on_posts(oct_2016_news_comments.rdd.map(lambda r: r), 2, 1)\n",
    "hillary_and_trump = res.filter(lambda r: ('trump' in r[1]) or ('hillary' in r[1]) or ('donald' in r[1]) or ('clinton' in r[1]))\n",
    "hillary_and_trump.toDF().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
